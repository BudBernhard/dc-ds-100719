{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra: Especially use of numpy for vectors and matrices. Difference between 1D and 2D arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication in numpy\n",
    "import numpy as np\n",
    "SIZE = 200\n",
    "A = np.random.rand(SIZE, SIZE)\n",
    "B = np.random.rand(SIZE, SIZE)\n",
    "out1 = A.dot(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://github.com/learn-co-curriculum/dsc-lineq-numpy-lab/tree/solution]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Apply linear algebra to fit a function to data, describing linear mappings between input and output variables\n",
    "/n Indicate how linear algebra is related to regression modeling](https://learn.co/tracks/data-science-career-v2/module-4-a-complete-data-science-project-using-multiple-regression/section-26-linear-algebra/regression-analysis-using-linear-algebra-and-numpy-code-along)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://github.com/learn-co-curriculum/dsc-linalg-regression-lab/tree/solution]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus and Cost functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def errors(x_values, y_values, m, b):\n",
    "    y_line = (b + m*x_values)\n",
    "    return (y_values - y_line)\n",
    "\n",
    "def squared_errors(x_values, y_values, m, b):\n",
    "    return np.round(errors(x_values, y_values, m, b)**2, 2)\n",
    "\n",
    "def residual_sum_squares(x_values, y_values, m, b):\n",
    "    return round(sum(squared_errors(x_values, y_values, m, b)), 2)\n",
    "\n",
    "def root_mean_squared_error(x_values, y_values, m, b):\n",
    "    return round(math.sqrt(sum(squared_errors(x_values, y_values, m, b)))/len(x_values), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    - What is gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to find the best fit line (m and b above)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updated_b(b, learning_rate, cost_curve_slope):\n",
    "    change_to_b = -1 * learning_rate * cost_curve_slope\n",
    "    return change_to_b + b\n",
    "\n",
    "def gradient_descent(x_values, y_values, steps, current_b, learning_rate, m):\n",
    "    cost_curve = []\n",
    "    for i in range(steps):\n",
    "        current_cost_slope = slope_at(x_values, y_values, m, current_b)['slope']\n",
    "        current_rss = residual_sum_squares(x_values, y_values, m, current_b)\n",
    "        cost_curve.append({'b': current_b, 'rss': round(current_rss,2), 'slope': round(current_cost_slope,2)})\n",
    "        current_b = updated_b(current_b, learning_rate, current_cost_slope)\n",
    "    return cost_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, m, numIterations):\n",
    "    xTrans = x.transpose()\n",
    "    costs = []\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = np.dot(x, theta)\n",
    "        loss = hypothesis - y\n",
    "        # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        cost = np.sum(loss ** 2) / (2 * m)\n",
    "        if i % (numIterations // 40) == 0:\n",
    "            print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "            print(theta)\n",
    "        costs.append(cost)\n",
    "        # avg gradient per example\n",
    "        gradient = np.dot(xTrans, loss) / m\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "    sns.scatterplot(y=costs, x=[i for i in range(len(costs))])\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[More complicated version](https://github.com/learn-co-curriculum/dsc-applying-gradient-descent-lab/tree/solution)\n",
    "\n",
    "[From Class](https://github.com/learn-co-students/dc-ds-100719/blob/master/module-4/week-1/Day-2-GradientDescent/Math_Gradient_Descent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is learning rate(step sizes) in gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha and \"The size of these steps is called the learning rate. With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension to linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Bias-Variance trade-off: Make sure you understand why this is a \"trade-off\".\n",
    "\n",
    "* Bias in Inverse of Variance \n",
    "* An underfitting line is an example of a high bias low variance model. \n",
    "* An overfitting line is an example of a high variance low bias model.\n",
    "* Low variance and Low bias models are the best\n",
    "* Variance is the amount by which the prediction function would change if we estimated it using a different training data set.\n",
    "* Bias is the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### - Polynomial regression how this might be relevant in the context of bias-variance trade-off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Polynomial regression allows for better fitting data that isn't well predicted using a linear model.\n",
    "* The risk of polynomial regressions is that it's easier to overfit data, so it's important to consider the Bias-Variance trade-off and perform proper cross-validation.\n",
    "* High variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bias-Variance Tradeoff - Lab](https://github.com/learn-co-students/dsc-bias-variance-trade-off-lab-dc-ds-100719/tree/solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1d8ca4cb9fec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Transform with MinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Transform the test data (X_test) using the same scaler:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Transform with MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "#Transform the test data (X_test) using the same scaler:\n",
    "# Scale the test set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Ridge and Lasso models: How are they similar to each other and how are they different. Also compare these models with linear regression. Understand Ridge and Lasso in the context of bias-variance. For example: \"Lasso decreases variance in the expense of adding a lit bit bias, etc.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/learn-co-students/dsc-ridge-and-lasso-regression-dc-ds-100719\n",
    "* https://github.com/learn-co-students/dsc-ridge-and-lasso-regression-lab-dc-ds-100719/tree/solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Feature selection: Forward and Backward future selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stepwise selection with p-values\n",
    "* https://github.com/learn-co-students/dsc-model-fit-linear-regression-dc-ds-100719\n",
    "* https://github.com/learn-co-students/dsc-model-fit-linear-regression-lab-dc-ds-100719/tree/solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build a linear regression model with interactions and polynomial features\n",
    "* [Use AIC and BIC to select the best value for the regularization parameter](https://github.com/learn-co-students/dsc-extensions-to-linear-models-lab-dc-ds-100719/tree/solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "## for correlation matrices\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "## for linear models\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Fitting the actual model\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X, hasconst=True )\n",
    "result = model.fit()\n",
    "labels = ['intercept'] + x_cols\n",
    "print(labels)\n",
    "result.summary(xname=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - ROC curves: especially understand how do we draw them by changing the threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/learn-co-students/dsc-roc-curves-and-auc-dc-ds-100719\n",
    "* [In this lab you further explored ROC curves and AUC, drawing graphs and then interpreting these results to lead to a more detailed and contextualized understanding of your model's accuracy. With Example Of Changing Threshold](https://github.com/learn-co-students/dsc-roc-curves-and-auc-lab-dc-ds-100719/tree/solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Understand the model: log-odds, likelihood function and maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Confusion matrices: recall-precision- True positive rate - False negative rate - F1 score.\n",
    "https://skymind.ai/wiki/accuracy-precision-recall-f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - AUC: how do you compare two different classification algorithm by AUC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is imbalanced dataset? What are the techniques to solve this problem?\n",
    "https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - BONUS \n",
    "https://www.reddit.com/r/learnmachinelearning/comments/8ic97h/what_is_one_hot_encoding_and_when_is_it_beneficial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
