{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:14:21.322592Z",
     "start_time": "2019-12-31T20:14:17.487803Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:14:35.792076Z",
     "start_time": "2019-12-31T20:14:29.942809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar2\n",
      "  Downloading https://files.pythonhosted.org/packages/16/68/adc395e0a3c86571081c8a2e2daaa5b58270f6854276a089a0e9b5fa2c33/progressbar2-3.47.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from progressbar2) (1.12.0)\n",
      "Collecting python-utils>=2.3.0 (from progressbar2)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
      "Installing collected packages: python-utils, progressbar2\n",
      "Successfully installed progressbar2-3.47.0 python-utils-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install progressbar2\n",
    "from progressbar import progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:16:12.849554Z",
     "start_time": "2019-12-31T20:16:12.841258Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to scrape reddit page (takes a reddit .json url)\n",
    "# returns posts \n",
    "\n",
    "def scraper_bike(url):\n",
    "    headers = {'User-Agent' : 'override this bad boy!'}\n",
    "    posts = []\n",
    "    after = {}\n",
    "\n",
    "    for page in progressbar(range(40)):\n",
    "        params = {'after': after}\n",
    "        pagepull = requests.get(url=url, params=params, headers=headers)\n",
    "        page_dict = pagepull.json()\n",
    "        posts.extend(page_dict['data']['children'])\n",
    "        after = page_dict['data']['after']\n",
    "        time.sleep(.2)\n",
    "        \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:16:14.185233Z",
     "start_time": "2019-12-31T20:16:14.175537Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to convert posts to DataFrame - won't allow duplicate posts since unique id 'name' is set as index\n",
    "# Extract: name (as index) and subreddit, selftext, title (as columns)\n",
    "\n",
    "def posts_to_df(post_list):\n",
    "    i = 0\n",
    "    post_dict = {}\n",
    "    \n",
    "    for post in post_list:\n",
    "        ind = post_list[i]['data']\n",
    "        post_dict[ind['name']] = [ind['subreddit'], ind['title'], ind['selftext']]\n",
    "        i += 1\n",
    "\n",
    "    df_name = pd.DataFrame(post_dict)\n",
    "    df_name = df_name.T\n",
    "    df_name.columns = ['subreddit', 'title', 'selftext']\n",
    "    \n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:16:27.853855Z",
     "start_time": "2019-12-31T20:16:27.850098Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes scraper function and url - outputs dataframe\n",
    "\n",
    "def scrape_to_df(scrape_func, url):\n",
    "    \n",
    "    return posts_to_df(scrape_func(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:16:31.003010Z",
     "start_time": "2019-12-31T20:16:30.994897Z"
    }
   },
   "outputs": [],
   "source": [
    "#### If you want to scrape repeatedly over time and add to a csv\n",
    "# scrape, import csv, concat, drop duplicate, and output to csv\n",
    "# takes in scraper function, url, csv filename to import, csv filename to output\n",
    "# Outputs - Concatenated DataFrame as csv\n",
    "\n",
    "def scrape_add(scrape_func, url, import_file, export_file):\n",
    "    scrape_df = posts_to_df(scrape_func(url))\n",
    "    imported_df = pd.read_csv(import_file, index_col = 'Unnamed: 0')\n",
    "    concat_df = pd.concat([imported_df, scrape_df])\n",
    "    concat_df = concat_df[~concat_df.index.duplicated(keep='first')]\n",
    "    concat_df.to_csv(export_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:19:12.989747Z",
     "start_time": "2019-12-31T20:18:25.761729Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (40 of 40) |########################| Elapsed Time: 0:00:24 Time:  0:00:24\n",
      "100% (40 of 40) |########################| Elapsed Time: 0:00:22 Time:  0:00:22\n"
     ]
    }
   ],
   "source": [
    "# You can also put in any 2 subreddits in as the URL and get results for those\n",
    "\n",
    "nfltest = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/nfl.json')\n",
    "nbatest = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/nba.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:23:49.018847Z",
     "start_time": "2019-12-31T20:22:59.790602Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (40 of 40) |########################| Elapsed Time: 0:00:23 Time:  0:00:23\n",
      "100% (40 of 40) |########################| Elapsed Time: 0:00:24 Time:  0:00:24\n"
     ]
    }
   ],
   "source": [
    "politics_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/politics.json')\n",
    "conservative_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/conservative.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:23:49.042741Z",
     "start_time": "2019-12-31T20:23:49.022864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(786, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbatest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:23:49.058000Z",
     "start_time": "2019-12-31T20:23:49.048075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(942, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfltest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These scrape_add functions add to already built csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:23:49.070485Z",
     "start_time": "2019-12-31T20:23:49.064444Z"
    }
   },
   "outputs": [],
   "source": [
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/CollegeBasketball/new.json', 'NCAA_Posts_Update2.csv', 'NCAA_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/AskScience/new.json', 'AskSci_Posts_Update2.csv', 'AskSci_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/nba/new.json', 'NBA_Posts_Update2.csv', 'NBA_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/nfl/new.json', 'NFL_Posts_Update2.csv', 'NFL_Posts_Update3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:24:46.544310Z",
     "start_time": "2019-12-31T20:24:43.911054Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:25:34.893343Z",
     "start_time": "2019-12-31T20:25:34.882919Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop column\n",
    "\n",
    "nfltest = nfltest.drop(columns = 'selftext')\n",
    "nbatest = nbatest.drop(columns = 'selftext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:25:40.002345Z",
     "start_time": "2019-12-31T20:25:39.991481Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge subreddit data\n",
    "\n",
    "train = pd.concat([nfltest, nbatest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:25:41.097020Z",
     "start_time": "2019-12-31T20:25:41.067907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t3_ei5he6</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Talko Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehwkan</th>\n",
       "      <td>nfl</td>\n",
       "      <td>/r/NFL Best Of 2019 Nomination Thread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei32ez</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Silver] The Ron Rivera deal is done. He has reached an agreement to become the new coach of the Redskins, according to a source familiar with negotiations. @AroundTheNFL @nflnetwork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei2x87</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter] Giants are expected to interview former Packers’ head coach Mike McCarthy this weekend, per source.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei7htz</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter]John Dorsey is out as the Browns’ GM, per source. Two sides couldn’t come to an agreement on a future re-structure of the organization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei3j1c</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter] Baylor HC Matt Rhule declined the chance to interview for the Cleveland Browns head coaching job, per league sources. Rhule is expected to be a leading candidate for the Giants’ and Panthers’ HC jobs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei3x5k</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter] Jaguars’ HC Doug Marrone is returning for the 2020 season, per sources.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei2us9</th>\n",
       "      <td>nfl</td>\n",
       "      <td>On the one hand, we're due a Super Bowl run from a team outside of the top two seeds. The last time was 2012.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei5tnj</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Serious] CJ Anderson says on Twitter that his brother has been killed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei4c4e</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Campbell] Bears GM Ryan Pace definitively commits to Mitch Trubisky as Bears QB1 in 2020.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei4crh</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter] Soon-to-be Redskins’ HC Ron Rivera is expected to target former Jaguars’ and Raiders’ HC Jack Del Rio as his new defensive coordinator in Washington, per sources. He is the leading candidate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei2lbn</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Yates] OT Jawaan Taylor was at one point during the pre-draft process considered a top 10 prospect. Medical concerns caused him to slip to the Jaguars at pick 35. He was the only rookie in the NFL to play 100% of the snaps for his team, showing improvement along the way. Impressive.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei5q5p</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Siciliano] Mike Tomlin: \"I'm comfortable with Mason Rudolph.\" Reporter: \"What about Devlin Hodges?\" Tomlin: \"Mason's the back up.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei2ad3</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Rapoport] # Cowboys owner Jerry Jones wanted 24-48 hours to let his decision marinate before officially moving on from coach Jason Garrett... so we wait.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei22el</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Pelissero] Chan Gailey is coming out of retirement to become the #Dolphins’ offensive coordinator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei4plc</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Lombardo] Dave Gettleman: We’ve hired four computer folks and redid the backend of our scouting department</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei29km</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Two years ago today, the playoff drought of 18 years in Buffalo ended. And it was all in thanks to the unlikeliest of heroes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei4j8k</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter] The Ron Rivera-Jack Del Rio team in Washington will be a strong one. In 2002, Del Rio took over the league’s worst defense in Carolina and turned it into the NFL’s second-ranked unit; in 2012 he took Broncos from 20th-ranked defense to second.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei3jbv</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[PFF] Nick Bosa had the most pressures ever by a rookie with 80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei49b4</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter] Cowboys and Jason Garrett are scheduled to meet at noon CT today, per sources. Resolution likely, but not definite. Expectation continues to be Garrett and Dallas will part ways, but the Cowboys are run as a family, Garrett has been a part of family, and it’s hard to part ways.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei3ds5</th>\n",
       "      <td>nfl</td>\n",
       "      <td>The Buccaneers and Browns were the only two teams to not make it to the playoffs this decade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei2f23</th>\n",
       "      <td>nfl</td>\n",
       "      <td>The /r/NFL thread from when Jason Garrett was Hired as the Dallas HC (and Wade Phillips fired)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei5h1i</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Slater] A source inside the Star tells me #Cowboys HC Jason Garrett has said goodbyes to several staff members while exchanging information to keep in touch. Late with Christmas cards or the end inevitable? Stay tuned.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei6y3q</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Rapoport] Browns GM John Dorsey has a meeting today with owner Jimmy Haslam and his job status appears to be in serious doubt, sources tell me and @MikeGarafolo. Everything is under evaluation after a disappointing season and that includes the GM.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei45ub</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Schefter] 49ers’ passing-game coordinator Mike LaFleur, the younger brother of Packers’ HC Matt LaFleur, is expected to interview this weekend for Browns’ head-coaching job, per source.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei3dck</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Rapoport] Source: #Ravens DC Don “Wink” Martindale was requested to interview for the #Giants head-coaching opportunity. He’d try to bring #LSU offensive guru Joe Brady with him -- an intriguing pair.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehwbjz</th>\n",
       "      <td>nfl</td>\n",
       "      <td>Dan Snyder once called defensive coordinator Mike Nolan’s scheme \"too vanilla,\" so he left a gallon of ice cream on Nolan's desk. Nolan chuckled. Later that season, Snyder sent more ice cream, three five-gallon drums this time, with the note: \"I wasn't joking. I do not like vanilla.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei55v6</th>\n",
       "      <td>nfl</td>\n",
       "      <td>AJ Brown Selected as PFF's Offensive Rookie of the Year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei5upt</th>\n",
       "      <td>nfl</td>\n",
       "      <td>If Patrick Mahomes goes 12-4 next year, his career winning percentage would go down.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei5vf8</th>\n",
       "      <td>nfl</td>\n",
       "      <td>[Highlight] “You don’t let that f**ker push you around, you’re too good! But you gotta keep your mind in the game and stay focused!” - Ron Rivera gives a fiery speech to his team down at halftime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egr7ay</th>\n",
       "      <td>nba</td>\n",
       "      <td>Heat fans, how good is Kendrick Nunn?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egyfuz</th>\n",
       "      <td>nba</td>\n",
       "      <td>GAME THREAD: Cleveland Cavaliers (9-22) @ Minnesota Timberwolves (11-19) - (December 29, 2019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egf21b</th>\n",
       "      <td>nba</td>\n",
       "      <td>Under Fizdale, Julius Randle was averaging 17/8.5/3.5 on 44/24/66. Under Milller he is averaging 22/9/2.6 on 47/37/76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_eh9od8</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Serious] How is Harden not even a top MVP candidate?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egd2nc</th>\n",
       "      <td>nba</td>\n",
       "      <td>Per Rick Carlisle: Last night’s Mavs/Spurs matchup was the first time in NBA history that two female coaches sat at the front of opposing benches at the same time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egmc8z</th>\n",
       "      <td>nba</td>\n",
       "      <td>Favorite basketball moments from this decade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egvh8k</th>\n",
       "      <td>nba</td>\n",
       "      <td>An Interesting Change to the NBA Playoffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egfx1z</th>\n",
       "      <td>nba</td>\n",
       "      <td>Reactions to the Kings signing Dewayne Dedmon to a 3yr/$41M contract only 5 months ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egt1oe</th>\n",
       "      <td>nba</td>\n",
       "      <td>Who has best chance to come out of hole Wolves or Suns?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehcwrf</th>\n",
       "      <td>nba</td>\n",
       "      <td>Why Stephen Curry (Not LeBron) Is the N.B.A. Player of the Decade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egtfs3</th>\n",
       "      <td>nba</td>\n",
       "      <td>I want to work for the NBA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egkwdc</th>\n",
       "      <td>nba</td>\n",
       "      <td>Fanbase Civil Wars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehdoh7</th>\n",
       "      <td>nba</td>\n",
       "      <td>Luka Doncic has mastered the game of Basketball at 20 years old.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egkk3l</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Highlight] Nunn throws the lob for the big Derrick Jones Jr. dunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egfkg5</th>\n",
       "      <td>nba</td>\n",
       "      <td>In game 7 of the 2016 NBA Finals Kyrie Irving had his jersey untucked when he entered the game prior to making his game winning three pointer. By the letter of the law this should have resulted in a delay of game penalty, would you want this to be called?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egw8ks</th>\n",
       "      <td>nba</td>\n",
       "      <td>Are the hornets still the least relevant franchise?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egs5o1</th>\n",
       "      <td>nba</td>\n",
       "      <td>The amazing story of the greatest AAU team you’ve never heard of. Featuring: Joe Dumars, Karl Malone, Hot Rod Williams, and Benny Anders.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehc8hy</th>\n",
       "      <td>nba</td>\n",
       "      <td>Current Nets &amp;gt; Current Warriors, KD + Kyrie &amp;gt; Steph + Klay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_eg59fx</th>\n",
       "      <td>nba</td>\n",
       "      <td>Zach Randolph retires after 17 seasons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egjdmd</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Highlight] Rick Carlisle's \"fucking cool\" play: KP lobs it to Dwight Powell for the and-one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_eghjzn</th>\n",
       "      <td>nba</td>\n",
       "      <td>L2M report concludes Rudy Gobert doinked Carmelo Anthony's arm on his game winning shot attempt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egikz1</th>\n",
       "      <td>nba</td>\n",
       "      <td>The last time the Celtics had two 30+ point scorers in the same game before tonight (Jayson Tatum and Jaylen Brown) was 2014 (Jeff Green and Avery Bradley)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egha2u</th>\n",
       "      <td>nba</td>\n",
       "      <td>Next season, LeBron James (currently 3rd, $306m) will overtake Kobe Bryant (2nd, $323m) and Kevin Garnett (1st, $334m) to become the NBA player with the highest career earnings ever.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_eghd11</th>\n",
       "      <td>nba</td>\n",
       "      <td>[Highlight] Jayson Tatum gets free and throws down the hammer!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_eh4img</th>\n",
       "      <td>nba</td>\n",
       "      <td>Who's better in each following group?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egpz42</th>\n",
       "      <td>nba</td>\n",
       "      <td>NBA Decade Awards of Reddittors - What's your opinion?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egogrx</th>\n",
       "      <td>nba</td>\n",
       "      <td>Embiid leads the Sixers in FT%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_egmt2s</th>\n",
       "      <td>nba</td>\n",
       "      <td>Six Heat players have scored 25 or more points multiple times this season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_eglnq6</th>\n",
       "      <td>nba</td>\n",
       "      <td>The Tale of 2011 Dirk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei88id</th>\n",
       "      <td>nba</td>\n",
       "      <td>Is Giannis pulling away from everyone else in the MVP race?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1728 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit  \\\n",
       "t3_ei5he6       nfl   \n",
       "t3_ehwkan       nfl   \n",
       "t3_ei32ez       nfl   \n",
       "t3_ei2x87       nfl   \n",
       "t3_ei7htz       nfl   \n",
       "t3_ei3j1c       nfl   \n",
       "t3_ei3x5k       nfl   \n",
       "t3_ei2us9       nfl   \n",
       "t3_ei5tnj       nfl   \n",
       "t3_ei4c4e       nfl   \n",
       "t3_ei4crh       nfl   \n",
       "t3_ei2lbn       nfl   \n",
       "t3_ei5q5p       nfl   \n",
       "t3_ei2ad3       nfl   \n",
       "t3_ei22el       nfl   \n",
       "t3_ei4plc       nfl   \n",
       "t3_ei29km       nfl   \n",
       "t3_ei4j8k       nfl   \n",
       "t3_ei3jbv       nfl   \n",
       "t3_ei49b4       nfl   \n",
       "t3_ei3ds5       nfl   \n",
       "t3_ei2f23       nfl   \n",
       "t3_ei5h1i       nfl   \n",
       "t3_ei6y3q       nfl   \n",
       "t3_ei45ub       nfl   \n",
       "t3_ei3dck       nfl   \n",
       "t3_ehwbjz       nfl   \n",
       "t3_ei55v6       nfl   \n",
       "t3_ei5upt       nfl   \n",
       "t3_ei5vf8       nfl   \n",
       "...             ...   \n",
       "t3_egr7ay       nba   \n",
       "t3_egyfuz       nba   \n",
       "t3_egf21b       nba   \n",
       "t3_eh9od8       nba   \n",
       "t3_egd2nc       nba   \n",
       "t3_egmc8z       nba   \n",
       "t3_egvh8k       nba   \n",
       "t3_egfx1z       nba   \n",
       "t3_egt1oe       nba   \n",
       "t3_ehcwrf       nba   \n",
       "t3_egtfs3       nba   \n",
       "t3_egkwdc       nba   \n",
       "t3_ehdoh7       nba   \n",
       "t3_egkk3l       nba   \n",
       "t3_egfkg5       nba   \n",
       "t3_egw8ks       nba   \n",
       "t3_egs5o1       nba   \n",
       "t3_ehc8hy       nba   \n",
       "t3_eg59fx       nba   \n",
       "t3_egjdmd       nba   \n",
       "t3_eghjzn       nba   \n",
       "t3_egikz1       nba   \n",
       "t3_egha2u       nba   \n",
       "t3_eghd11       nba   \n",
       "t3_eh4img       nba   \n",
       "t3_egpz42       nba   \n",
       "t3_egogrx       nba   \n",
       "t3_egmt2s       nba   \n",
       "t3_eglnq6       nba   \n",
       "t3_ei88id       nba   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                       title  \n",
       "t3_ei5he6                                                                                                                                                                                                                                                                                      Talko Tuesday  \n",
       "t3_ehwkan                                                                                                                                                                                                                                                              /r/NFL Best Of 2019 Nomination Thread  \n",
       "t3_ei32ez                                                                                                             [Silver] The Ron Rivera deal is done. He has reached an agreement to become the new coach of the Redskins, according to a source familiar with negotiations. @AroundTheNFL @nflnetwork  \n",
       "t3_ei2x87                                                                                                                                                                                     [Schefter] Giants are expected to interview former Packers’ head coach Mike McCarthy this weekend, per source.  \n",
       "t3_ei7htz                                                                                                                                                  [Schefter]John Dorsey is out as the Browns’ GM, per source. Two sides couldn’t come to an agreement on a future re-structure of the organization.  \n",
       "t3_ei3j1c                                                                                [Schefter] Baylor HC Matt Rhule declined the chance to interview for the Cleveland Browns head coaching job, per league sources. Rhule is expected to be a leading candidate for the Giants’ and Panthers’ HC jobs.  \n",
       "t3_ei3x5k                                                                                                                                                                                                                 [Schefter] Jaguars’ HC Doug Marrone is returning for the 2020 season, per sources.  \n",
       "t3_ei2us9                                                                                                                                                                                      On the one hand, we're due a Super Bowl run from a team outside of the top two seeds. The last time was 2012.  \n",
       "t3_ei5tnj                                                                                                                                                                                                                             [Serious] CJ Anderson says on Twitter that his brother has been killed  \n",
       "t3_ei4c4e                                                                                                                                                                                                         [Campbell] Bears GM Ryan Pace definitively commits to Mitch Trubisky as Bears QB1 in 2020.  \n",
       "t3_ei4crh                                                                                         [Schefter] Soon-to-be Redskins’ HC Ron Rivera is expected to target former Jaguars’ and Raiders’ HC Jack Del Rio as his new defensive coordinator in Washington, per sources. He is the leading candidate.  \n",
       "t3_ei2lbn       [Yates] OT Jawaan Taylor was at one point during the pre-draft process considered a top 10 prospect. Medical concerns caused him to slip to the Jaguars at pick 35. He was the only rookie in the NFL to play 100% of the snaps for his team, showing improvement along the way. Impressive.  \n",
       "t3_ei5q5p                                                                                                                                                                [Siciliano] Mike Tomlin: \"I'm comfortable with Mason Rudolph.\" Reporter: \"What about Devlin Hodges?\" Tomlin: \"Mason's the back up.\"  \n",
       "t3_ei2ad3                                                                                                                                         [Rapoport] # Cowboys owner Jerry Jones wanted 24-48 hours to let his decision marinate before officially moving on from coach Jason Garrett... so we wait.  \n",
       "t3_ei22el                                                                                                                                                                                                 [Pelissero] Chan Gailey is coming out of retirement to become the #Dolphins’ offensive coordinator  \n",
       "t3_ei4plc                                                                                                                                                                                        [Lombardo] Dave Gettleman: We’ve hired four computer folks and redid the backend of our scouting department  \n",
       "t3_ei29km                                                                                                                                                                      Two years ago today, the playoff drought of 18 years in Buffalo ended. And it was all in thanks to the unlikeliest of heroes.  \n",
       "t3_ei4j8k                                     [Schefter] The Ron Rivera-Jack Del Rio team in Washington will be a strong one. In 2002, Del Rio took over the league’s worst defense in Carolina and turned it into the NFL’s second-ranked unit; in 2012 he took Broncos from 20th-ranked defense to second.  \n",
       "t3_ei3jbv                                                                                                                                                                                                                                    [PFF] Nick Bosa had the most pressures ever by a rookie with 80  \n",
       "t3_ei49b4  [Schefter] Cowboys and Jason Garrett are scheduled to meet at noon CT today, per sources. Resolution likely, but not definite. Expectation continues to be Garrett and Dallas will part ways, but the Cowboys are run as a family, Garrett has been a part of family, and it’s hard to part ways.  \n",
       "t3_ei3ds5                                                                                                                                                                                                       The Buccaneers and Browns were the only two teams to not make it to the playoffs this decade  \n",
       "t3_ei2f23                                                                                                                                                                                                     The /r/NFL thread from when Jason Garrett was Hired as the Dallas HC (and Wade Phillips fired)  \n",
       "t3_ei5h1i                                                                        [Slater] A source inside the Star tells me #Cowboys HC Jason Garrett has said goodbyes to several staff members while exchanging information to keep in touch. Late with Christmas cards or the end inevitable? Stay tuned.  \n",
       "t3_ei6y3q                                           [Rapoport] Browns GM John Dorsey has a meeting today with owner Jimmy Haslam and his job status appears to be in serious doubt, sources tell me and @MikeGarafolo. Everything is under evaluation after a disappointing season and that includes the GM.  \n",
       "t3_ei45ub                                                                                                         [Schefter] 49ers’ passing-game coordinator Mike LaFleur, the younger brother of Packers’ HC Matt LaFleur, is expected to interview this weekend for Browns’ head-coaching job, per source.  \n",
       "t3_ei3dck                                                                                          [Rapoport] Source: #Ravens DC Don “Wink” Martindale was requested to interview for the #Giants head-coaching opportunity. He’d try to bring #LSU offensive guru Joe Brady with him -- an intriguing pair.  \n",
       "t3_ehwbjz       Dan Snyder once called defensive coordinator Mike Nolan’s scheme \"too vanilla,\" so he left a gallon of ice cream on Nolan's desk. Nolan chuckled. Later that season, Snyder sent more ice cream, three five-gallon drums this time, with the note: \"I wasn't joking. I do not like vanilla.\"  \n",
       "t3_ei55v6                                                                                                                                                                                                                                            AJ Brown Selected as PFF's Offensive Rookie of the Year  \n",
       "t3_ei5upt                                                                                                                                                                                                               If Patrick Mahomes goes 12-4 next year, his career winning percentage would go down.  \n",
       "t3_ei5vf8                                                                                                [Highlight] “You don’t let that f**ker push you around, you’re too good! But you gotta keep your mind in the game and stay focused!” - Ron Rivera gives a fiery speech to his team down at halftime  \n",
       "...                                                                                                                                                                                                                                                                                                      ...  \n",
       "t3_egr7ay                                                                                                                                                                                                                                                              Heat fans, how good is Kendrick Nunn?  \n",
       "t3_egyfuz                                                                                                                                                                                                     GAME THREAD: Cleveland Cavaliers (9-22) @ Minnesota Timberwolves (11-19) - (December 29, 2019)  \n",
       "t3_egf21b                                                                                                                                                                              Under Fizdale, Julius Randle was averaging 17/8.5/3.5 on 44/24/66. Under Milller he is averaging 22/9/2.6 on 47/37/76  \n",
       "t3_eh9od8                                                                                                                                                                                                                                              [Serious] How is Harden not even a top MVP candidate?  \n",
       "t3_egd2nc                                                                                                                                Per Rick Carlisle: Last night’s Mavs/Spurs matchup was the first time in NBA history that two female coaches sat at the front of opposing benches at the same time.  \n",
       "t3_egmc8z                                                                                                                                                                                                                                                       Favorite basketball moments from this decade  \n",
       "t3_egvh8k                                                                                                                                                                                                                                                          An Interesting Change to the NBA Playoffs  \n",
       "t3_egfx1z                                                                                                                                                                                                             Reactions to the Kings signing Dewayne Dedmon to a 3yr/$41M contract only 5 months ago  \n",
       "t3_egt1oe                                                                                                                                                                                                                                            Who has best chance to come out of hole Wolves or Suns?  \n",
       "t3_ehcwrf                                                                                                                                                                                                                                  Why Stephen Curry (Not LeBron) Is the N.B.A. Player of the Decade  \n",
       "t3_egtfs3                                                                                                                                                                                                                                                                         I want to work for the NBA  \n",
       "t3_egkwdc                                                                                                                                                                                                                                                                                 Fanbase Civil Wars  \n",
       "t3_ehdoh7                                                                                                                                                                                                                                   Luka Doncic has mastered the game of Basketball at 20 years old.  \n",
       "t3_egkk3l                                                                                                                                                                                                                                 [Highlight] Nunn throws the lob for the big Derrick Jones Jr. dunk  \n",
       "t3_egfkg5                                    In game 7 of the 2016 NBA Finals Kyrie Irving had his jersey untucked when he entered the game prior to making his game winning three pointer. By the letter of the law this should have resulted in a delay of game penalty, would you want this to be called?  \n",
       "t3_egw8ks                                                                                                                                                                                                                                                Are the hornets still the least relevant franchise?  \n",
       "t3_egs5o1                                                                                                                                                          The amazing story of the greatest AAU team you’ve never heard of. Featuring: Joe Dumars, Karl Malone, Hot Rod Williams, and Benny Anders.  \n",
       "t3_ehc8hy                                                                                                                                                                                                                                  Current Nets &gt; Current Warriors, KD + Kyrie &gt; Steph + Klay.  \n",
       "t3_eg59fx                                                                                                                                                                                                                                                             Zach Randolph retires after 17 seasons  \n",
       "t3_egjdmd                                                                                                                                                                                                       [Highlight] Rick Carlisle's \"fucking cool\" play: KP lobs it to Dwight Powell for the and-one  \n",
       "t3_eghjzn                                                                                                                                                                                                    L2M report concludes Rudy Gobert doinked Carmelo Anthony's arm on his game winning shot attempt  \n",
       "t3_egikz1                                                                                                                                        The last time the Celtics had two 30+ point scorers in the same game before tonight (Jayson Tatum and Jaylen Brown) was 2014 (Jeff Green and Avery Bradley)  \n",
       "t3_egha2u                                                                                                             Next season, LeBron James (currently 3rd, $306m) will overtake Kobe Bryant (2nd, $323m) and Kevin Garnett (1st, $334m) to become the NBA player with the highest career earnings ever.  \n",
       "t3_eghd11                                                                                                                                                                                                                                     [Highlight] Jayson Tatum gets free and throws down the hammer!  \n",
       "t3_eh4img                                                                                                                                                                                                                                                              Who's better in each following group?  \n",
       "t3_egpz42                                                                                                                                                                                                                                             NBA Decade Awards of Reddittors - What's your opinion?  \n",
       "t3_egogrx                                                                                                                                                                                                                                                                     Embiid leads the Sixers in FT%  \n",
       "t3_egmt2s                                                                                                                                                                                                                          Six Heat players have scored 25 or more points multiple times this season  \n",
       "t3_eglnq6                                                                                                                                                                                                                                                                              The Tale of 2011 Dirk  \n",
       "t3_ei88id                                                                                                                                                                                                                                        Is Giannis pulling away from everyone else in the MVP race?  \n",
       "\n",
       "[1728 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize (grab only word characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:27:58.666071Z",
     "start_time": "2019-12-31T20:27:58.659200Z"
    }
   },
   "outputs": [],
   "source": [
    "word_tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:29:40.989306Z",
     "start_time": "2019-12-31T20:29:40.963571Z"
    }
   },
   "outputs": [],
   "source": [
    "train['title'] = train['title'].map(lambda x: word_tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:29:41.297910Z",
     "start_time": "2019-12-31T20:29:41.287745Z"
    }
   },
   "outputs": [],
   "source": [
    "# rejoin list of tokenized words into single string for each row\n",
    "\n",
    "train['title'] = train['title'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:29:42.039694Z",
     "start_time": "2019-12-31T20:29:42.029489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t3_ei5he6                                                                                                                                                                      talko tuesday\n",
       "t3_ehwkan                                                                                                                                               r nfl best of 2019 nomination thread\n",
       "t3_ei32ez    silver the ron rivera deal is done he has reached an agreement to become the new coach of the redskins according to a source familiar with negotiations aroundthenfl nflnetwork\n",
       "t3_ei2x87                                                                          schefter giants are expected to interview former packers head coach mike mccarthy this weekend per source\n",
       "t3_ei7htz                                       schefter john dorsey is out as the browns gm per source two sides couldn t come to an agreement on a future re structure of the organization\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:29:42.808500Z",
     "start_time": "2019-12-31T20:29:42.801442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t3_ei5he6                                                                                                                                                                      talko tuesday\n",
       "t3_ehwkan                                                                                                                                               r nfl best of 2019 nomination thread\n",
       "t3_ei32ez    silver the ron rivera deal is done he has reached an agreement to become the new coach of the redskins according to a source familiar with negotiations aroundthenfl nflnetwork\n",
       "t3_ei2x87                                                                          schefter giants are expected to interview former packers head coach mike mccarthy this weekend per source\n",
       "t3_ei7htz                                       schefter john dorsey is out as the browns gm per source two sides couldn t come to an agreement on a future re structure of the organization\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split and converting series to list of strings then to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:29:49.571344Z",
     "start_time": "2019-12-31T20:29:49.563402Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train[['title']]\n",
    "y = train['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:30:21.567200Z",
     "start_time": "2019-12-31T20:30:21.556078Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:31:28.169359Z",
     "start_time": "2019-12-31T20:31:28.110938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    0.545139\n",
       "nba    0.454861\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline is\n",
    "\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:31:49.877022Z",
     "start_time": "2019-12-31T20:31:49.873601Z"
    }
   },
   "outputs": [],
   "source": [
    "# create our training data list - this is a list of strings, with each string being a post title\n",
    "\n",
    "clean_train_data = []\n",
    "\n",
    "for traindata in X_train['title']:\n",
    "    clean_train_data.append(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:31:50.884041Z",
     "start_time": "2019-12-31T20:31:50.875709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1296"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:31:52.615090Z",
     "start_time": "2019-12-31T20:31:52.607025Z"
    }
   },
   "outputs": [],
   "source": [
    "# create test data list\n",
    "\n",
    "clean_test_data = []\n",
    "\n",
    "for testdata in X_test['title']:\n",
    "    clean_test_data.append(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:31:53.737206Z",
     "start_time": "2019-12-31T20:31:53.730718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:32:00.439967Z",
     "start_time": "2019-12-31T20:32:00.436406Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate our CountVectorizer. This counts the number of appearances of all the words in our training data and\n",
    "# eliminates common english stop words. 5000 max features works well for our purposes (tested various numbers). Our\n",
    "# data is already preprocessed and tokenized manually earlier. ngram_range is 1,3, although all or nearly all our\n",
    "# features are single words\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words='english',\n",
    "                             max_features=5000,\n",
    "                             ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:35:55.008869Z",
     "start_time": "2019-12-31T20:35:54.861366Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit our training data and test data lists to our count_vectorizer\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:35:55.305708Z",
     "start_time": "2019-12-31T20:35:55.290065Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert to array\n",
    "\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:35:55.956844Z",
     "start_time": "2019-12-31T20:35:55.949307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1296, 5000), (432, 5000))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shapes\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:35:56.613043Z",
     "start_time": "2019-12-31T20:35:56.605015Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I wanted check that the features corpus was as expected - removed print statement for readability\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:35:57.746539Z",
     "start_time": "2019-12-31T20:35:57.738786Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '000 bonus',\n",
       " '000 yards',\n",
       " '10',\n",
       " '10 22',\n",
       " '10 22 defeat',\n",
       " '10 23',\n",
       " '10 consequential',\n",
       " '10 consequential athletes',\n",
       " '10 december',\n",
       " '10 december 29',\n",
       " '10 defeat',\n",
       " '10 los',\n",
       " '10 los angeles',\n",
       " '10 rebs',\n",
       " '100',\n",
       " '1000',\n",
       " '1000 yard',\n",
       " '100th',\n",
       " '102',\n",
       " '104',\n",
       " '10th',\n",
       " '11',\n",
       " '11 13',\n",
       " '11 20',\n",
       " '11 98',\n",
       " '11 98 97',\n",
       " '11 field',\n",
       " '11 miami',\n",
       " '11 miami heat',\n",
       " '11 point',\n",
       " '110',\n",
       " '112',\n",
       " '113',\n",
       " '113 point',\n",
       " '113 point differential',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '12',\n",
       " '12 20',\n",
       " '12 21',\n",
       " '12 22',\n",
       " '12 27',\n",
       " '12 27 2019',\n",
       " '12 28',\n",
       " '12 28 vs',\n",
       " '12 29',\n",
       " '12 37',\n",
       " '12 sacks',\n",
       " '12 sacks adrian',\n",
       " '120',\n",
       " '122',\n",
       " '123',\n",
       " '129',\n",
       " '12th',\n",
       " '12th head',\n",
       " '12th head coach',\n",
       " '13',\n",
       " '13 20',\n",
       " '13 21',\n",
       " '13 games',\n",
       " '13 record',\n",
       " '13 team',\n",
       " '14',\n",
       " '14 18',\n",
       " '14 18 december',\n",
       " '14 19',\n",
       " '141',\n",
       " '14th',\n",
       " '15',\n",
       " '15 defeat',\n",
       " '15th',\n",
       " '16',\n",
       " '16 15',\n",
       " '16 game',\n",
       " '16 game season',\n",
       " '17',\n",
       " '17 defeat',\n",
       " '17 seasons',\n",
       " '18',\n",
       " '18 amp',\n",
       " '18 december',\n",
       " '18 points',\n",
       " '18 points assists',\n",
       " '18th',\n",
       " '19',\n",
       " '19 december',\n",
       " '19 points',\n",
       " '1978',\n",
       " '1978 patriots',\n",
       " '1985',\n",
       " '1987',\n",
       " '1990',\n",
       " '1992',\n",
       " '1994',\n",
       " '1999',\n",
       " '1st',\n",
       " '1st amp',\n",
       " '20',\n",
       " '20 12',\n",
       " '20 december',\n",
       " '20 december 31',\n",
       " '20 defeat',\n",
       " '20 points',\n",
       " '20 touchdowns',\n",
       " '20 years',\n",
       " '20 years old',\n",
       " '200',\n",
       " '2000',\n",
       " '2000s',\n",
       " '2002',\n",
       " '2007',\n",
       " '2009',\n",
       " '2010',\n",
       " '2010s',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2016 complete',\n",
       " '2016 complete 70',\n",
       " '2016 season',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '2019 12',\n",
       " '2019 12 28',\n",
       " '2019 moves',\n",
       " '2019 moves za',\n",
       " '2019 nfl',\n",
       " '2019 regular',\n",
       " '2019 regular season',\n",
       " '2019 season',\n",
       " '2020',\n",
       " '2020 nfl',\n",
       " '2020s',\n",
       " '206',\n",
       " '20th',\n",
       " '21',\n",
       " '21 10',\n",
       " '21 11',\n",
       " '21 12',\n",
       " '21 points',\n",
       " '22',\n",
       " '22 10',\n",
       " '22 11',\n",
       " '22 81',\n",
       " '22 december',\n",
       " '22 december 30',\n",
       " '22 defeat',\n",
       " '23',\n",
       " '23 11',\n",
       " '23 december',\n",
       " '23 defeat',\n",
       " '23 defeat indiana',\n",
       " '23 shooting',\n",
       " '24',\n",
       " '24 defeat',\n",
       " '24 hours',\n",
       " '24 points',\n",
       " '24 total',\n",
       " '243',\n",
       " '25',\n",
       " '250',\n",
       " '250 000',\n",
       " '250 000 bonus',\n",
       " '26',\n",
       " '27',\n",
       " '27 2019',\n",
       " '28',\n",
       " '28 2019',\n",
       " '28 vs',\n",
       " '29',\n",
       " '29 2019',\n",
       " '2nd',\n",
       " '2nd seed',\n",
       " '30',\n",
       " '30 2019',\n",
       " '30 point',\n",
       " '30 points',\n",
       " '30 years',\n",
       " '30 years ago',\n",
       " '300',\n",
       " '303',\n",
       " '303 points',\n",
       " '31',\n",
       " '31 2019',\n",
       " '31 pts',\n",
       " '32',\n",
       " '32 players',\n",
       " '32 players averaging',\n",
       " '32 points',\n",
       " '3296',\n",
       " '33',\n",
       " '34']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:41:08.243649Z",
     "start_time": "2019-12-31T20:41:08.236072Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:41:09.895321Z",
     "start_time": "2019-12-31T20:41:09.886976Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit logistic regression model\n",
    "\n",
    "lr = LogisticRegression(penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:41:10.513895Z",
     "start_time": "2019-12-31T20:41:10.506676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1296, 5000), (1296,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape check\n",
    "\n",
    "train_data_features.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:41:11.315362Z",
     "start_time": "2019-12-31T20:41:11.003486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:41:11.762155Z",
     "start_time": "2019-12-31T20:41:11.719794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9953703703703703"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:41:12.237650Z",
     "start_time": "2019-12-31T20:41:12.225894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a dataframe that matches features to coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:44:10.115326Z",
     "start_time": "2019-12-31T20:44:10.110532Z"
    }
   },
   "outputs": [],
   "source": [
    "coef_list = lr.coef_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:44:10.388283Z",
     "start_time": "2019-12-31T20:44:10.381867Z"
    }
   },
   "outputs": [],
   "source": [
    "coef_list = coef_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:44:10.863782Z",
     "start_time": "2019-12-31T20:44:10.854705Z"
    }
   },
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'features': vectorizer.get_feature_names(),\n",
    "                        'coefs': coef_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:44:11.340515Z",
     "start_time": "2019-12-31T20:44:11.323209Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>nba</td>\n",
       "      <td>-2.466467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>heat</td>\n",
       "      <td>-1.322496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>harden</td>\n",
       "      <td>-1.226266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>warriors</td>\n",
       "      <td>-1.195970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>lebron</td>\n",
       "      <td>-1.108881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>kat</td>\n",
       "      <td>-1.080393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>luka</td>\n",
       "      <td>-1.063615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>okc</td>\n",
       "      <td>-0.942514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4538</th>\n",
       "      <td>suns</td>\n",
       "      <td>-0.888450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>points</td>\n",
       "      <td>-0.851006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>lakers</td>\n",
       "      <td>-0.821345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>clippers</td>\n",
       "      <td>-0.811701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>bucks</td>\n",
       "      <td>-0.800685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>basketball</td>\n",
       "      <td>-0.799819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>highlights</td>\n",
       "      <td>-0.791372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>kings</td>\n",
       "      <td>-0.776714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>duos</td>\n",
       "      <td>-0.771216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4715</th>\n",
       "      <td>tonight</td>\n",
       "      <td>-0.762593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>hawks</td>\n",
       "      <td>-0.757049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>knicks</td>\n",
       "      <td>-0.742876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>celtics</td>\n",
       "      <td>-0.731853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>james</td>\n",
       "      <td>-0.713254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4389</th>\n",
       "      <td>shooting</td>\n",
       "      <td>-0.710755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>kawhi</td>\n",
       "      <td>-0.681222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>star</td>\n",
       "      <td>-0.675913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>milwaukee</td>\n",
       "      <td>-0.674865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2010</td>\n",
       "      <td>-0.672965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>beverley</td>\n",
       "      <td>-0.672703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>draymond</td>\n",
       "      <td>-0.639745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>curry</td>\n",
       "      <td>-0.636807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>field</td>\n",
       "      <td>0.779464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>chargers</td>\n",
       "      <td>0.785406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>rookie</td>\n",
       "      <td>0.794490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>packers</td>\n",
       "      <td>0.799688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>raiders</td>\n",
       "      <td>0.813660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>final</td>\n",
       "      <td>0.820387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3938</th>\n",
       "      <td>redskins</td>\n",
       "      <td>0.822342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4810</th>\n",
       "      <td>value</td>\n",
       "      <td>0.836388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4939</th>\n",
       "      <td>yard</td>\n",
       "      <td>0.849559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>jets</td>\n",
       "      <td>0.898334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>49ers</td>\n",
       "      <td>0.905835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>nfc</td>\n",
       "      <td>0.915311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4729</th>\n",
       "      <td>touchdown</td>\n",
       "      <td>0.919437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4308</th>\n",
       "      <td>says</td>\n",
       "      <td>0.921492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>division</td>\n",
       "      <td>0.941115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2019</td>\n",
       "      <td>0.954683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>rivera</td>\n",
       "      <td>0.955936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>dolphins</td>\n",
       "      <td>0.987586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>giants</td>\n",
       "      <td>1.066490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>saints</td>\n",
       "      <td>1.084530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4335</th>\n",
       "      <td>seahawks</td>\n",
       "      <td>1.167771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>patriots</td>\n",
       "      <td>1.175454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4577</th>\n",
       "      <td>td</td>\n",
       "      <td>1.190764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>broncos</td>\n",
       "      <td>1.191109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>eagles</td>\n",
       "      <td>1.201023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>cowboys</td>\n",
       "      <td>1.217012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>qb</td>\n",
       "      <td>1.243371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4943</th>\n",
       "      <td>yards</td>\n",
       "      <td>1.305362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>browns</td>\n",
       "      <td>1.342860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>nfl</td>\n",
       "      <td>2.387893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        features     coefs\n",
       "2330         nba -2.466467\n",
       "1402        heat -1.322496\n",
       "1365      harden -1.226266\n",
       "4844    warriors -1.195970\n",
       "1780      lebron -1.108881\n",
       "1664         kat -1.080393\n",
       "2172        luka -1.063615\n",
       "2444         okc -0.942514\n",
       "4538        suns -0.888450\n",
       "2943      points -0.851006\n",
       "1729      lakers -0.821345\n",
       "699     clippers -0.811701\n",
       "577        bucks -0.800685\n",
       "459   basketball -0.799819\n",
       "1443  highlights -0.791372\n",
       "1692       kings -0.776714\n",
       "1015        duos -0.771216\n",
       "4715     tonight -0.762593\n",
       "1375       hawks -0.757049\n",
       "1706      knicks -0.742876\n",
       "637      celtics -0.731853\n",
       "1567       james -0.713254\n",
       "4389    shooting -0.710755\n",
       "1667       kawhi -0.681222\n",
       "4483        star -0.675913\n",
       "2279   milwaukee -0.674865\n",
       "116         2010 -0.672965\n",
       "489     beverley -0.672703\n",
       "999     draymond -0.639745\n",
       "806        curry -0.636807\n",
       "...          ...       ...\n",
       "1130       field  0.779464\n",
       "655     chargers  0.785406\n",
       "4225      rookie  0.794490\n",
       "2486     packers  0.799688\n",
       "3501     raiders  0.813660\n",
       "1136       final  0.820387\n",
       "3938    redskins  0.822342\n",
       "4810       value  0.836388\n",
       "4939        yard  0.849559\n",
       "1603        jets  0.898334\n",
       "227        49ers  0.905835\n",
       "2369         nfc  0.915311\n",
       "4729   touchdown  0.919437\n",
       "4308        says  0.921492\n",
       "952     division  0.941115\n",
       "129         2019  0.954683\n",
       "4205      rivera  0.955936\n",
       "966     dolphins  0.987586\n",
       "1277      giants  1.066490\n",
       "4285      saints  1.084530\n",
       "4335    seahawks  1.167771\n",
       "2531    patriots  1.175454\n",
       "4577          td  1.190764\n",
       "552      broncos  1.191109\n",
       "1019      eagles  1.201023\n",
       "782      cowboys  1.217012\n",
       "3291          qb  1.243371\n",
       "4943       yards  1.305362\n",
       "560       browns  1.342860\n",
       "2375         nfl  2.387893\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values(by = ['coefs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's throw out these unfair words and rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:44:55.098559Z",
     "start_time": "2019-12-31T20:44:55.087637Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "extra_stopwords = ['nba', 'basketball', 'football', 'nfl']\n",
    "\n",
    "stopwords.update(extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:44:56.588703Z",
     "start_time": "2019-12-31T20:44:56.389424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1296, 5000), (432, 5000))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = stopwords,\n",
    "                             max_features = 5000,\n",
    "                             ngram_range = (1, 3))\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:44:57.487630Z",
     "start_time": "2019-12-31T20:44:57.226513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:44:58.034695Z",
     "start_time": "2019-12-31T20:44:57.992244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9915123456790124"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:44:58.680919Z",
     "start_time": "2019-12-31T20:44:58.672658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9421296296296297"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:44:59.216009Z",
     "start_time": "2019-12-31T20:44:59.192878Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>heat</td>\n",
       "      <td>-1.390041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>warriors</td>\n",
       "      <td>-1.225633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>harden</td>\n",
       "      <td>-1.177751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1866</th>\n",
       "      <td>lebron</td>\n",
       "      <td>-1.105873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>luka</td>\n",
       "      <td>-1.101096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>kat</td>\n",
       "      <td>-1.072424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>okc</td>\n",
       "      <td>-0.950810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>points</td>\n",
       "      <td>-0.901830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>kings</td>\n",
       "      <td>-0.874735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>tonight</td>\n",
       "      <td>-0.861127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>suns</td>\n",
       "      <td>-0.859219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>bucks</td>\n",
       "      <td>-0.830743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4912</th>\n",
       "      <td>would</td>\n",
       "      <td>-0.827413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>lakers</td>\n",
       "      <td>-0.799734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>james</td>\n",
       "      <td>-0.759617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>knicks</td>\n",
       "      <td>-0.755210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>shooting</td>\n",
       "      <td>-0.745149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>clippers</td>\n",
       "      <td>-0.733114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>highlights</td>\n",
       "      <td>-0.729627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>bulls</td>\n",
       "      <td>-0.728976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2010</td>\n",
       "      <td>-0.728782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4428</th>\n",
       "      <td>star</td>\n",
       "      <td>-0.723765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>duos</td>\n",
       "      <td>-0.699848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>paul</td>\n",
       "      <td>-0.677946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>hawks</td>\n",
       "      <td>-0.675446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>celtics</td>\n",
       "      <td>-0.664825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>december</td>\n",
       "      <td>-0.655968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>averaging</td>\n",
       "      <td>-0.655171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>draymond</td>\n",
       "      <td>-0.643391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>curry</td>\n",
       "      <td>-0.634090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>redskins</td>\n",
       "      <td>0.803469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>rookie</td>\n",
       "      <td>0.808711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4492</th>\n",
       "      <td>super</td>\n",
       "      <td>0.824038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>schefter</td>\n",
       "      <td>0.830613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>playoffs</td>\n",
       "      <td>0.838459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>joe</td>\n",
       "      <td>0.840766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4936</th>\n",
       "      <td>yard</td>\n",
       "      <td>0.844929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>packers</td>\n",
       "      <td>0.860093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>division</td>\n",
       "      <td>0.866768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>nfc</td>\n",
       "      <td>0.899685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>jets</td>\n",
       "      <td>0.920368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>touchdown</td>\n",
       "      <td>0.921184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>final</td>\n",
       "      <td>0.921870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>raiders</td>\n",
       "      <td>0.940284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>says</td>\n",
       "      <td>0.950209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>rivera</td>\n",
       "      <td>0.985933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>49ers</td>\n",
       "      <td>0.998656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>dolphins</td>\n",
       "      <td>1.011286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>giants</td>\n",
       "      <td>1.110441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4530</th>\n",
       "      <td>td</td>\n",
       "      <td>1.115988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4210</th>\n",
       "      <td>saints</td>\n",
       "      <td>1.148052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>broncos</td>\n",
       "      <td>1.157141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>seahawks</td>\n",
       "      <td>1.190951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>cowboys</td>\n",
       "      <td>1.244741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>eagles</td>\n",
       "      <td>1.244755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2019</td>\n",
       "      <td>1.250071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4940</th>\n",
       "      <td>yards</td>\n",
       "      <td>1.285090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>patriots</td>\n",
       "      <td>1.292955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>browns</td>\n",
       "      <td>1.347323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>qb</td>\n",
       "      <td>1.353911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        features     coefs\n",
       "1471        heat -1.390041\n",
       "4828    warriors -1.225633\n",
       "1436      harden -1.177751\n",
       "1866      lebron -1.105873\n",
       "2081        luka -1.101096\n",
       "1731         kat -1.072424\n",
       "2697         okc -0.950810\n",
       "2920      points -0.901830\n",
       "1759       kings -0.874735\n",
       "4676     tonight -0.861127\n",
       "4487        suns -0.859219\n",
       "605        bucks -0.830743\n",
       "4912       would -0.827413\n",
       "1795      lakers -0.799734\n",
       "1636       james -0.759617\n",
       "1773      knicks -0.755210\n",
       "4319    shooting -0.745149\n",
       "729     clippers -0.733114\n",
       "1512  highlights -0.729627\n",
       "611        bulls -0.728976\n",
       "116         2010 -0.728782\n",
       "4428        star -0.723765\n",
       "1030        duos -0.699848\n",
       "2807        paul -0.677946\n",
       "1444       hawks -0.675446\n",
       "667      celtics -0.664825\n",
       "898     december -0.655968\n",
       "451    averaging -0.655171\n",
       "1014    draymond -0.643391\n",
       "837        curry -0.634090\n",
       "...          ...       ...\n",
       "3793    redskins  0.803469\n",
       "4148      rookie  0.808711\n",
       "4492       super  0.824038\n",
       "4239    schefter  0.830613\n",
       "2908    playoffs  0.838459\n",
       "1682         joe  0.840766\n",
       "4936        yard  0.844929\n",
       "2756     packers  0.860093\n",
       "978     division  0.866768\n",
       "2634         nfc  0.899685\n",
       "1671        jets  0.920368\n",
       "4704   touchdown  0.921184\n",
       "1160       final  0.921870\n",
       "3651     raiders  0.940284\n",
       "4233        says  0.950209\n",
       "4128      rivera  0.985933\n",
       "228        49ers  0.998656\n",
       "985     dolphins  1.011286\n",
       "1345      giants  1.110441\n",
       "4530          td  1.115988\n",
       "4210      saints  1.148052\n",
       "580      broncos  1.157141\n",
       "4259    seahawks  1.190951\n",
       "813      cowboys  1.244741\n",
       "1034      eagles  1.244755\n",
       "130         2019  1.250071\n",
       "4940       yards  1.285090\n",
       "2803    patriots  1.292955\n",
       "589       browns  1.347323\n",
       "3440          qb  1.353911\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_list = lr.coef_.tolist()\n",
    "coef_list = coef_list[0]\n",
    "\n",
    "coef_df = pd.DataFrame({'features' : vectorizer.get_feature_names(),\n",
    "                       'coefs' : coef_list})\n",
    "\n",
    "coef_df.sort_values(by = ['coefs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:45:55.383472Z",
     "start_time": "2019-12-31T20:45:55.088405Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:45:55.605856Z",
     "start_time": "2019-12-31T20:45:55.599750Z"
    }
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:45:59.748032Z",
     "start_time": "2019-12-31T20:45:56.270068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:45:59.783370Z",
     "start_time": "2019-12-31T20:45:59.750029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992283950617284"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:45:59.795717Z",
     "start_time": "2019-12-31T20:45:59.785802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8194444444444444"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:45:59.853061Z",
     "start_time": "2019-12-31T20:45:59.800094Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:46:21.988072Z",
     "start_time": "2019-12-31T20:46:21.980117Z"
    }
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:47:15.249117Z",
     "start_time": "2019-12-31T20:46:22.409938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:47:16.713773Z",
     "start_time": "2019-12-31T20:47:15.250114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992283950617284"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:47:17.043375Z",
     "start_time": "2019-12-31T20:47:16.715913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8819444444444444"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Matrix on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:47:24.399444Z",
     "start_time": "2019-12-31T20:47:24.396743Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:47:25.349149Z",
     "start_time": "2019-12-31T20:47:25.346338Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:47:25.973703Z",
     "start_time": "2019-12-31T20:47:25.968104Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:47:26.562535Z",
     "start_time": "2019-12-31T20:47:26.556148Z"
    }
   },
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm,\n",
    "                    columns=['predict_neg', 'predict_pos'],\n",
    "                    index = ['actual_neg', 'actual_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:47:27.104549Z",
     "start_time": "2019-12-31T20:47:27.096065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_neg</th>\n",
       "      <th>predict_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_neg</th>\n",
       "      <td>188</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_pos</th>\n",
       "      <td>16</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predict_neg  predict_pos\n",
       "actual_neg          188            9\n",
       "actual_pos           16          219"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking where our model failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:48:52.942913Z",
     "start_time": "2019-12-31T20:48:52.935933Z"
    }
   },
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({'y_actual' : y_test,\n",
    "             'y_predicted' : y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:48:53.366044Z",
     "start_time": "2019-12-31T20:48:53.361043Z"
    }
   },
   "outputs": [],
   "source": [
    "mismatch_df = comparison_df[comparison_df['y_actual'] != comparison_df['y_predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:48:53.613411Z",
     "start_time": "2019-12-31T20:48:53.595752Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "mismatch2_df = pd.concat([mismatch_df, X_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:48:53.782237Z",
     "start_time": "2019-12-31T20:48:53.774975Z"
    }
   },
   "outputs": [],
   "source": [
    "# All incorrect predictions with titles\n",
    "\n",
    "mismatches = mismatch2_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:48:55.236264Z",
     "start_time": "2019-12-31T20:48:55.222609Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_actual</th>\n",
       "      <th>y_predicted</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t3_egvgl9</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>charania the nuggets has been open to discussing deals around young talents malik beasley and juancho hernangomez but have asked for a high value ahead of the february trade deadline according to sources both turned down extensions this past summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehboan</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>highlight tavon wilson called for unnecessary roughness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehc6su</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>aside from the 2015 16 season what is the best nba season in nba history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehgzu1</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>siciliano so game 256 comes down to one play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehiiue</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl draft order top 20 set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehlj3x</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>which playoff game s will romo cast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehm1xy</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>fans of the 8 teams playing this weekend how does everyone feel about their first round matchup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehnm3w</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>jackson xavien howard arrest details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehnwvg</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>person tre boston on cam newton the rest of those boys are playing to 40 but they got o lines why does cam get to have just average lines get beat up amp the rest of these guys have healthy o lines and get the same treatment as aw man these guys can play til they re 40 with ease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehpy1s</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>stupid question why did the lakers sign cousins to a one year contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehqbt5</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>official r nba power rankings 5 12 30 2019 last ranking of the decade edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehr9ma</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>2010s in review the top 10 stat leaders over the decade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehrs31</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>what are the nfl s most expensive seats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehrygk</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>john wall rehab update dunking basketballs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehsttx</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>what is hayward s value on the market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehtrhb</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>if you re an eagles fan and can get one injured player back for the seattle game who would it be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehuiic</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>schlecht steven adams said on the lowe post that he was disappointed in kd for not reaching out to him or his teammates before after he left when asked if he s talked to him since adams replied not at all i sent him a message on facebook but i don t think he uses facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehupzl</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>where do good new head coaches come from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehx3ii</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>why is the epa of a successfully recovered onside kick 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehyxn4</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>must see playoff games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ehzu6h</th>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>if a newly signed player like kd did not play a single game in his first year of the contract does he still receive his salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei36pa</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>this week s top r nfl highlight s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei5he6</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>talko tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei7c16</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>kremenjas rams got 25 4m in cap space currently they ll have 54 1m if they make these moves release eric weddle saves 4 25m restructure jared goff frees 16 8m trade todd gurley saves 4 65m extend jalen ramsey frees roughly 3m at least 3 4 should be easy to do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ei82kw</th>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>big ben approaching significant date in elbow rehab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y_actual y_predicted  \\\n",
       "t3_egvgl9      nba         nfl   \n",
       "t3_ehboan      nfl         nba   \n",
       "t3_ehc6su      nba         nfl   \n",
       "t3_ehgzu1      nfl         nba   \n",
       "t3_ehiiue      nfl         nba   \n",
       "t3_ehlj3x      nfl         nba   \n",
       "t3_ehm1xy      nfl         nba   \n",
       "t3_ehnm3w      nfl         nba   \n",
       "t3_ehnwvg      nfl         nba   \n",
       "t3_ehpy1s      nba         nfl   \n",
       "t3_ehqbt5      nba         nfl   \n",
       "t3_ehr9ma      nfl         nba   \n",
       "t3_ehrs31      nfl         nba   \n",
       "t3_ehrygk      nba         nfl   \n",
       "t3_ehsttx      nba         nfl   \n",
       "t3_ehtrhb      nfl         nba   \n",
       "t3_ehuiic      nba         nfl   \n",
       "t3_ehupzl      nfl         nba   \n",
       "t3_ehx3ii      nfl         nba   \n",
       "t3_ehyxn4      nba         nfl   \n",
       "t3_ehzu6h      nba         nfl   \n",
       "t3_ei36pa      nfl         nba   \n",
       "t3_ei5he6      nfl         nba   \n",
       "t3_ei7c16      nfl         nba   \n",
       "t3_ei82kw      nfl         nba   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                             title  \n",
       "t3_egvgl9                                 charania the nuggets has been open to discussing deals around young talents malik beasley and juancho hernangomez but have asked for a high value ahead of the february trade deadline according to sources both turned down extensions this past summer  \n",
       "t3_ehboan                                                                                                                                                                                                                                  highlight tavon wilson called for unnecessary roughness  \n",
       "t3_ehc6su                                                                                                                                                                                                                 aside from the 2015 16 season what is the best nba season in nba history  \n",
       "t3_ehgzu1                                                                                                                                                                                                                                             siciliano so game 256 comes down to one play  \n",
       "t3_ehiiue                                                                                                                                                                                                                                                               nfl draft order top 20 set  \n",
       "t3_ehlj3x                                                                                                                                                                                                                                                      which playoff game s will romo cast  \n",
       "t3_ehm1xy                                                                                                                                                                                          fans of the 8 teams playing this weekend how does everyone feel about their first round matchup  \n",
       "t3_ehnm3w                                                                                                                                                                                                                                                     jackson xavien howard arrest details  \n",
       "t3_ehnwvg  person tre boston on cam newton the rest of those boys are playing to 40 but they got o lines why does cam get to have just average lines get beat up amp the rest of these guys have healthy o lines and get the same treatment as aw man these guys can play til they re 40 with ease  \n",
       "t3_ehpy1s                                                                                                                                                                                                                   stupid question why did the lakers sign cousins to a one year contract  \n",
       "t3_ehqbt5                                                                                                                                                                                                            official r nba power rankings 5 12 30 2019 last ranking of the decade edition  \n",
       "t3_ehr9ma                                                                                                                                                                                                                                  2010s in review the top 10 stat leaders over the decade  \n",
       "t3_ehrs31                                                                                                                                                                                                                                                  what are the nfl s most expensive seats  \n",
       "t3_ehrygk                                                                                                                                                                                                                                               john wall rehab update dunking basketballs  \n",
       "t3_ehsttx                                                                                                                                                                                                                                                    what is hayward s value on the market  \n",
       "t3_ehtrhb                                                                                                                                                                                         if you re an eagles fan and can get one injured player back for the seattle game who would it be  \n",
       "t3_ehuiic         schlecht steven adams said on the lowe post that he was disappointed in kd for not reaching out to him or his teammates before after he left when asked if he s talked to him since adams replied not at all i sent him a message on facebook but i don t think he uses facebook  \n",
       "t3_ehupzl                                                                                                                                                                                                                                                 where do good new head coaches come from  \n",
       "t3_ehx3ii                                                                                                                                                                                                                                 why is the epa of a successfully recovered onside kick 0  \n",
       "t3_ehyxn4                                                                                                                                                                                                                                                                   must see playoff games  \n",
       "t3_ehzu6h                                                                                                                                                           if a newly signed player like kd did not play a single game in his first year of the contract does he still receive his salary  \n",
       "t3_ei36pa                                                                                                                                                                                                                                                        this week s top r nfl highlight s  \n",
       "t3_ei5he6                                                                                                                                                                                                                                                                            talko tuesday  \n",
       "t3_ei7c16                      kremenjas rams got 25 4m in cap space currently they ll have 54 1m if they make these moves release eric weddle saves 4 25m restructure jared goff frees 16 8m trade todd gurley saves 4 65m extend jalen ramsey frees roughly 3m at least 3 4 should be easy to do  \n",
       "t3_ei82kw                                                                                                                                                                                                                                      big ben approaching significant date in elbow rehab  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency / Inverse Document Frequency\n",
    "\n",
    "TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF(w) = log_e(Total number of documents / Number of documents with term w in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:52:42.036962Z",
     "start_time": "2019-12-31T20:52:42.029808Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(analyzer=\"word\",\n",
    "                            tokenizer=None,\n",
    "                            preprocessor=None,\n",
    "                            stop_words=['nba', 'nfl', 'football', 'basketball'],\n",
    "                            max_features=5000,\n",
    "                            ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:52:43.367264Z",
     "start_time": "2019-12-31T20:52:43.118087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1296, 5000), (432, 5000))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features = tfidf_vec.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = tfidf_vec.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:52:43.494940Z",
     "start_time": "2019-12-31T20:52:43.369580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:52:43.535368Z",
     "start_time": "2019-12-31T20:52:43.504267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9699074074074074"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:52:43.774758Z",
     "start_time": "2019-12-31T20:52:43.763899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try on some other subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:52:44.896977Z",
     "start_time": "2019-12-31T20:52:44.889922Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.concat([politics_test, conservative_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:52:45.081321Z",
     "start_time": "2019-12-31T20:52:45.072580Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train[['title']]\n",
    "y = train['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:52:45.555135Z",
     "start_time": "2019-12-31T20:52:45.551419Z"
    }
   },
   "outputs": [],
   "source": [
    "# politics_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/politics.json')\n",
    "# conservative_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/conservative.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:52:46.062723Z",
     "start_time": "2019-12-31T20:52:46.051270Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:52:46.506758Z",
     "start_time": "2019-12-31T20:52:46.478420Z"
    }
   },
   "outputs": [],
   "source": [
    "politics_test = politics_test.drop(columns='selftext')\n",
    "conservative_test = conservative_test.drop(columns='selftext')\n",
    "\n",
    "train = pd.concat([politics_test, conservative_test])\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "train['title'] = train['title'].map(lambda x: tokenizer.tokenize(x.lower()))\n",
    "train['title'] = train['title'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:52:47.564932Z",
     "start_time": "2019-12-31T20:52:47.560294Z"
    }
   },
   "outputs": [],
   "source": [
    "# create our training data list - this is a list of strings, with each string being a post title\n",
    "\n",
    "clean_train_data = []\n",
    "\n",
    "for traindata in X_train['title']:\n",
    "    clean_train_data.append(traindata)\n",
    "    \n",
    "    \n",
    "# create test data list\n",
    "\n",
    "clean_test_data = []\n",
    "\n",
    "for testdata in X_test['title']:\n",
    "    clean_test_data.append(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:53:22.716397Z",
     "start_time": "2019-12-31T20:53:22.599068Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words='english',\n",
    "                             max_features=5000,\n",
    "                             ngram_range=(2, 3))\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:53:23.568627Z",
     "start_time": "2019-12-31T20:53:23.564620Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:53:23.973133Z",
     "start_time": "2019-12-31T20:53:23.965587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1343, 5000), (1343,))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:53:24.539306Z",
     "start_time": "2019-12-31T20:53:24.237677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9538346984363366"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)\n",
    "\n",
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:53:24.692074Z",
     "start_time": "2019-12-31T20:53:24.682952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6629464285714286"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:53:25.269880Z",
     "start_time": "2019-12-31T20:53:25.266522Z"
    }
   },
   "outputs": [],
   "source": [
    "coef_list = lr.coef_.tolist()\n",
    "\n",
    "coef_list = coef_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T20:53:26.037858Z",
     "start_time": "2019-12-31T20:53:26.012304Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>merry christmas</td>\n",
       "      <td>-1.191564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>hunter biden</td>\n",
       "      <td>-0.930663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>working class</td>\n",
       "      <td>-0.841837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>hillary clinton</td>\n",
       "      <td>-0.772845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>lisa murkowski</td>\n",
       "      <td>-0.744156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>christmas message</td>\n",
       "      <td>-0.681478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>fake news</td>\n",
       "      <td>-0.667685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>articles impeachment</td>\n",
       "      <td>-0.665513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>anti trump</td>\n",
       "      <td>-0.661449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>advanced society</td>\n",
       "      <td>-0.650944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>hanukkah attack</td>\n",
       "      <td>-0.646619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4610</th>\n",
       "      <td>trump supporters</td>\n",
       "      <td>-0.644229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>grover cleveland</td>\n",
       "      <td>-0.631360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>trump impeached</td>\n",
       "      <td>-0.576335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>rachel maddow</td>\n",
       "      <td>-0.572757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>adam schiff</td>\n",
       "      <td>-0.570645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>biden says</td>\n",
       "      <td>-0.570579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>illegal aliens</td>\n",
       "      <td>-0.561930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394</th>\n",
       "      <td>pelosi impeachment</td>\n",
       "      <td>-0.558331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4340</th>\n",
       "      <td>second amendment</td>\n",
       "      <td>-0.557562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>getting wrong</td>\n",
       "      <td>-0.543721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>social justice</td>\n",
       "      <td>-0.539021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>anti semitic</td>\n",
       "      <td>-0.529588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>looking 2019</td>\n",
       "      <td>-0.514621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>reportedly beginning think</td>\n",
       "      <td>-0.509103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>reportedly beginning</td>\n",
       "      <td>-0.509103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>fisa court</td>\n",
       "      <td>-0.506760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>guy gun</td>\n",
       "      <td>-0.487889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4377</th>\n",
       "      <td>skip school</td>\n",
       "      <td>-0.483781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>house seats</td>\n",
       "      <td>-0.479992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>biden iraq</td>\n",
       "      <td>0.677702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788</th>\n",
       "      <td>pompeo visit</td>\n",
       "      <td>0.686031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>ukraine whistleblower</td>\n",
       "      <td>0.690005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>george conway</td>\n",
       "      <td>0.690426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000 voters</td>\n",
       "      <td>0.696340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>embassy iraq</td>\n",
       "      <td>0.706669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020 democrats</td>\n",
       "      <td>0.710568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>new year</td>\n",
       "      <td>0.722865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>fears trump</td>\n",
       "      <td>0.725608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>nazi salute</td>\n",
       "      <td>0.742890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>peter strzok</td>\n",
       "      <td>0.758572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>trump says</td>\n",
       "      <td>0.795539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4932</th>\n",
       "      <td>wanted son</td>\n",
       "      <td>0.797626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4620</th>\n",
       "      <td>trump tariffs</td>\n",
       "      <td>0.809801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4476</th>\n",
       "      <td>trump climate</td>\n",
       "      <td>0.821647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4415</th>\n",
       "      <td>susan collins</td>\n",
       "      <td>0.826016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>federal judge</td>\n",
       "      <td>0.849884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>giuliani associate</td>\n",
       "      <td>0.900445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>new hampshire</td>\n",
       "      <td>0.910823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>rudy giuliani</td>\n",
       "      <td>0.950633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>donald trump</td>\n",
       "      <td>0.955857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>red flag</td>\n",
       "      <td>0.963018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>white house</td>\n",
       "      <td>1.034154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4794</th>\n",
       "      <td>ukraine aid</td>\n",
       "      <td>1.148298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>andrew yang</td>\n",
       "      <td>1.209785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>alleged whistleblower</td>\n",
       "      <td>1.216754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4463</th>\n",
       "      <td>trump administration</td>\n",
       "      <td>1.265837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>health care</td>\n",
       "      <td>1.370452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>john lewis</td>\n",
       "      <td>1.424275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>bernie sanders</td>\n",
       "      <td>1.600041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        features     coefs\n",
       "1077             merry christmas -1.191564\n",
       "417                 hunter biden -0.930663\n",
       "4971               working class -0.841837\n",
       "409              hillary clinton -0.772845\n",
       "539               lisa murkowski -0.744156\n",
       "170            christmas message -0.681478\n",
       "303                    fake news -0.667685\n",
       "74          articles impeachment -0.665513\n",
       "71                    anti trump -0.661449\n",
       "34              advanced society -0.650944\n",
       "395              hanukkah attack -0.646619\n",
       "4610            trump supporters -0.644229\n",
       "385             grover cleveland -0.631360\n",
       "4488             trump impeached -0.576335\n",
       "3422               rachel maddow -0.572757\n",
       "29                   adam schiff -0.570645\n",
       "108                   biden says -0.570579\n",
       "425               illegal aliens -0.561930\n",
       "2394          pelosi impeachment -0.558331\n",
       "4340            second amendment -0.557562\n",
       "351                getting wrong -0.543721\n",
       "4378              social justice -0.539021\n",
       "66                  anti semitic -0.529588\n",
       "626                 looking 2019 -0.514621\n",
       "3884  reportedly beginning think -0.509103\n",
       "3883        reportedly beginning -0.509103\n",
       "320                   fisa court -0.506760\n",
       "391                      guy gun -0.487889\n",
       "4377                 skip school -0.483781\n",
       "415                  house seats -0.479992\n",
       "...                          ...       ...\n",
       "104                   biden iraq  0.677702\n",
       "2788                pompeo visit  0.686031\n",
       "4797       ukraine whistleblower  0.690005\n",
       "342                george conway  0.690426\n",
       "5                     000 voters  0.696340\n",
       "284                 embassy iraq  0.706669\n",
       "15                2020 democrats  0.710568\n",
       "1707                    new year  0.722865\n",
       "311                  fears trump  0.725608\n",
       "1547                 nazi salute  0.742890\n",
       "2525                peter strzok  0.758572\n",
       "4555                  trump says  0.795539\n",
       "4932                  wanted son  0.797626\n",
       "4620               trump tariffs  0.809801\n",
       "4476               trump climate  0.821647\n",
       "4415               susan collins  0.826016\n",
       "316                federal judge  0.849884\n",
       "354           giuliani associate  0.900445\n",
       "1661               new hampshire  0.910823\n",
       "4282               rudy giuliani  0.950633\n",
       "265                 donald trump  0.955857\n",
       "3660                    red flag  0.963018\n",
       "4952                 white house  1.034154\n",
       "4794                 ukraine aid  1.148298\n",
       "60                   andrew yang  1.209785\n",
       "46         alleged whistleblower  1.216754\n",
       "4463        trump administration  1.265837\n",
       "401                  health care  1.370452\n",
       "473                   john lewis  1.424275\n",
       "100               bernie sanders  1.600041\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df = pd.DataFrame({'features' : vectorizer.get_feature_names(),\n",
    "                       'coefs' : coef_list})\n",
    "\n",
    "coef_df.sort_values(by = ['coefs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... for, like, actual poets. By [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "\n",
    "## Why word vectors?\n",
    "\n",
    "Poetry is, at its core, the art of identifying and manipulating linguistic similarity. I have discovered a truly marvelous proof of this, which this notebook is too narrow to contain. (By which I mean: I will elaborate on this some other time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animal similarity and simple linear algebra\n",
    "\n",
    "We'll begin by considering a small subset of English: words for animals. Our task is to be able to write computer programs to find similarities among these words and the creatures they designate. To do this, we might start by making a spreadsheet of some animals and their characteristics. For example:\n",
    "\n",
    "![Animal spreadsheet](http://static.decontextualize.com/snaps/animal-spreadsheet.png)\n",
    "\n",
    "This spreadsheet associates a handful of animals with two numbers: their cuteness and their size, both in a range from zero to one hundred. (The values themselves are simply based on my own judgment. Your taste in cuteness and evaluation of size may differ significantly from mine. As with all data, these data are simply a mirror reflection of the person who collected them.)\n",
    "\n",
    "These values give us everything we need to make determinations about which animals are similar (at least, similar in the properties that we've included in the data). Try to answer the following question: Which animal is most similar to a capybara? You could go through the values one by one and do the math to make that evaluation, but visualizing the data as points in 2-dimensional space makes finding the answer very intuitive:\n",
    "\n",
    "![Animal space](http://static.decontextualize.com/snaps/animal-space.png)\n",
    "\n",
    "The plot shows us that the closest animal to the capybara is the panda bear (again, in terms of its subjective size and cuteness). One way of calculating how \"far apart\" two points are is to find their *Euclidean distance*. (This is simply the length of the line that connects the two points.) For points in two dimensions, Euclidean distance can be calculated with the following Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:02:39.440954Z",
     "start_time": "2019-12-31T21:02:39.437049Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def distance2d(x1, y1, x2, y2):\n",
    "    return np.linalg.norm(np.array([x1, y1])-np.array([x2, y2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the distance between \"capybara\" (70, 30) and \"panda\" (74, 40):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:02:40.242873Z",
     "start_time": "2019-12-31T21:02:40.235337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.180339887498949"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance2d(70, 30, 75, 40) # panda and capybara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... is less than the distance between \"tarantula\" and \"elephant\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:02:41.114187Z",
     "start_time": "2019-12-31T21:02:41.108771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.0096149401583"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance2d(8, 3, 65, 90) # tarantula and elephant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling animals in this way has a few other interesting properties. For example, you can pick an arbitrary point in \"animal space\" and then find the animal closest to that point. If you imagine an animal of size 25 and cuteness 30, you can easily look at the space to find the animal that most closely fits that description: the chicken.\n",
    "\n",
    "Reasoning visually, you can also answer questions like: what's halfway between a chicken and an elephant? Simply draw a line from \"elephant\" to \"chicken,\" mark off the midpoint and find the closest animal. (According to our chart, halfway between an elephant and a chicken is a horse.)\n",
    "\n",
    "You can also ask: what's the *difference* between a hamster and a tarantula? According to our plot, it's about seventy five units of cute (and a few units of size).\n",
    "\n",
    "The relationship of \"difference\" is an interesting one, because it allows us to reason about *analogous* relationships. In the chart below, I've drawn an arrow from \"tarantula\" to \"hamster\" (in blue):\n",
    "\n",
    "![Animal analogy](http://static.decontextualize.com/snaps/animal-space-analogy.png)\n",
    "\n",
    "You can understand this arrow as being the *relationship* between a tarantula and a hamster, in terms of their size and cuteness (i.e., hamsters and tarantulas are about the same size, but hamsters are much cuter). In the same diagram, I've also transposed this same arrow (this time in red) so that its origin point is \"chicken.\" The arrow ends closest to \"kitten.\" What we've discovered is that the animal that is about the same size as a chicken but much cuter is... a kitten. To put it in terms of an analogy:\n",
    "\n",
    "    Tarantulas are to hamsters as chickens are to kittens.\n",
    "    \n",
    "A sequence of numbers used to identify a point is called a *vector*, and the kind of math we've been doing so far is called *linear algebra.* (Linear algebra is surprisingly useful across many domains: It's the same kind of math you might do to, e.g., simulate the velocity and acceleration of a sprite in a video game.)\n",
    "\n",
    "A set of vectors that are all part of the same data set is often called a *vector space*. The vector space of animals in this section has two *dimensions*, by which I mean that each vector in the space has two numbers associated with it (i.e., two columns in the spreadsheet). The fact that this space has two dimensions just happens to make it easy to *visualize* the space by drawing a 2D plot. But most vector spaces you'll work with will have more than two dimensions—sometimes many hundreds. In those cases, it's more difficult to visualize the \"space,\" but the math works pretty much the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language with vectors: colors\n",
    "\n",
    "So far, so good. We have a system in place—albeit highly subjective—for talking about animals and the words used to name them. I want to talk about another vector space that has to do with language: the vector space of colors.\n",
    "\n",
    "Colors are often represented in computers as vectors with three dimensions: red, green, and blue. Just as with the animals in the previous section, we can use these vectors to answer questions like: which colors are similar? What's the most likely color name for an arbitrarily chosen set of values for red, green and blue? Given the names of two colors, what's the name of those colors' \"average\"?\n",
    "\n",
    "We'll be working with this [color data](https://github.com/dariusk/corpora/blob/master/data/colors/xkcd.json) from the [xkcd color survey](https://blog.xkcd.com/2010/05/03/color-survey-results/). The data relates a color name to the RGB value associated with that color. [Here's a page that shows what the colors look like](https://xkcd.com/color/rgb/). Download the color data and put it in the same directory as this notebook.\n",
    "\n",
    "A few notes before we proceed:\n",
    "\n",
    "* The linear algebra functions implemented below (`addv`, `meanv`, etc.) are slow, potentially inaccurate, and shouldn't be used for \"real\" code—I wrote them so beginner programmers can understand how these kinds of functions work behind the scenes. Use [numpy](http://www.numpy.org/) for fast and accurate math in Python.\n",
    "* If you're interested in perceptually accurate color math in Python, consider using the [colormath library](http://python-colormath.readthedocs.io/en/latest/).\n",
    "\n",
    "Now, import the `json` library and load the color data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:11:09.362564Z",
     "start_time": "2019-12-31T21:11:09.358918Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:11:09.666590Z",
     "start_time": "2019-12-31T21:11:09.609226Z"
    }
   },
   "outputs": [],
   "source": [
    "resp = requests.get('https://raw.githubusercontent.com/dariusk/corpora/master/data/colors/xkcd.json')\n",
    "color_data = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function converts colors from hex format (`#1a2b3c`) to a tuple of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:11:10.030810Z",
     "start_time": "2019-12-31T21:11:10.025264Z"
    }
   },
   "outputs": [],
   "source": [
    "def hex_to_int(s):\n",
    "    return int(s[1:3], 16), int(s[3:5], 16), int(s[5:7], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following cell creates a dictionary and populates it with mappings from color names to RGB vectors for each color in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:11:10.568291Z",
     "start_time": "2019-12-31T21:11:10.559538Z"
    }
   },
   "outputs": [],
   "source": [
    "colors = dict()\n",
    "for item in color_data['colors']:\n",
    "    colors[item[\"color\"]] = hex_to_int(item[\"hex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:11:11.023201Z",
     "start_time": "2019-12-31T21:11:11.018122Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 117, 14)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors['olive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:11:11.228192Z",
     "start_time": "2019-12-31T21:11:11.218058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 0, 0)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors['red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:11:11.438769Z",
     "start_time": "2019-12-31T21:11:11.432195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors['black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:11:11.656211Z",
     "start_time": "2019-12-31T21:11:11.651224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cloudy blue', 'dark pastel green', 'dust', 'electric lime', 'fresh green', 'light eggplant', 'nasty green', 'really light blue', 'tea', 'warm purple', 'yellowish tan', 'cement', 'dark grass green', 'dusty teal', 'grey teal', 'macaroni and cheese', 'pinkish tan', 'spruce', 'strong blue', 'toxic green', 'windows blue', 'blue blue', 'blue with a hint of purple', 'booger', 'bright sea green', 'dark green blue', 'deep turquoise', 'green teal', 'strong pink', 'bland', 'deep aqua', 'lavender pink', 'light moss green', 'light seafoam green', 'olive yellow', 'pig pink', 'deep lilac', 'desert', 'dusty lavender', 'purpley grey', 'purply', 'candy pink', 'light pastel green', 'boring green', 'kiwi green', 'light grey green', 'orange pink', 'tea green', 'very light brown', 'egg shell', 'eggplant purple', 'powder pink', 'reddish grey', 'baby shit brown', 'liliac', 'stormy blue', 'ugly brown', 'custard', 'darkish pink', 'deep brown', 'greenish beige', 'manilla', 'off blue', 'battleship grey', 'browny green', 'bruise', 'kelley green', 'sickly yellow', 'sunny yellow', 'azul', 'darkgreen', 'green/yellow', 'lichen', 'light light green', 'pale gold', 'sun yellow', 'tan green', 'burple', 'butterscotch', 'toupe', 'dark cream', 'indian red', 'light lavendar', 'poison green', 'baby puke green', 'bright yellow green', 'charcoal grey', 'squash', 'cinnamon', 'light pea green', 'radioactive green', 'raw sienna', 'baby purple', 'cocoa', 'light royal blue', 'orangeish', 'rust brown', 'sand brown', 'swamp', 'tealish green', 'burnt siena', 'camo', 'dusk blue', 'fern', 'old rose', 'pale light green', 'peachy pink', 'rosy pink', 'light bluish green', 'light bright green', 'light neon green', 'light seafoam', 'tiffany blue', 'washed out green', 'browny orange', 'nice blue', 'sapphire', 'greyish teal', 'orangey yellow', 'parchment', 'straw', 'very dark brown', 'terracota', 'ugly blue', 'clear blue', 'creme', 'foam green', 'grey/green', 'light gold', 'seafoam blue', 'topaz', 'violet pink', 'wintergreen', 'yellow tan', 'dark fuchsia', 'indigo blue', 'light yellowish green', 'pale magenta', 'rich purple', 'sunflower yellow', 'green/blue', 'leather', 'racing green', 'vivid purple', 'dark royal blue', 'hazel', 'muted pink', 'booger green', 'canary', 'cool grey', 'dark taupe', 'darkish purple', 'true green', 'coral pink', 'dark sage', 'dark slate blue', 'flat blue', 'mushroom', 'rich blue', 'dirty purple', 'greenblue', 'icky green', 'light khaki', 'warm blue', 'dark hot pink', 'deep sea blue', 'carmine', 'dark yellow green', 'pale peach', 'plum purple', 'golden rod', 'neon red', 'old pink', 'very pale blue', 'blood orange', 'grapefruit', 'sand yellow', 'clay brown', 'dark blue grey', 'flat green', 'light green blue', 'warm pink', 'dodger blue', 'gross green', 'ice', 'metallic blue', 'pale salmon', 'sap green', 'algae', 'bluey grey', 'greeny grey', 'highlighter green', 'light light blue', 'light mint', 'raw umber', 'vivid blue', 'deep lavender', 'dull teal', 'light greenish blue', 'mud green', 'pinky', 'red wine', 'shit green', 'tan brown', 'darkblue', 'rosa', 'lipstick', 'pale mauve', 'claret', 'dandelion', 'orangered', 'poop green', 'ruby', 'dark', 'greenish turquoise', 'pastel red', 'piss yellow', 'bright cyan', 'dark coral', 'algae green', 'darkish red', 'reddy brown', 'blush pink', 'camouflage green', 'lawn green', 'putty', 'vibrant blue', 'dark sand', 'purple/blue', 'saffron', 'twilight', 'warm brown', 'bluegrey', 'bubble gum pink', 'duck egg blue', 'greenish cyan', 'petrol', 'royal', 'butter', 'dusty orange', 'off yellow', 'pale olive green', 'orangish', 'leaf', 'light blue grey', 'dried blood', 'lightish purple', 'rusty red', 'lavender blue', 'light grass green', 'light mint green', 'sunflower', 'velvet', 'brick orange', 'lightish red', 'pure blue', 'twilight blue', 'violet red', 'yellowy brown', 'carnation', 'muddy yellow', 'dark seafoam green', 'deep rose', 'dusty red', 'grey/blue', 'lemon lime', 'purple/pink', 'brown yellow', 'purple brown', 'wisteria', 'banana yellow', 'lipstick red', 'water blue', 'brown grey', 'vibrant purple', 'baby green', 'barf green', 'eggshell blue', 'sandy yellow', 'cool green', 'pale', 'blue/grey', 'hot magenta', 'greyblue', 'purpley', 'baby shit green', 'brownish pink', 'dark aquamarine', 'diarrhea', 'light mustard', 'pale sky blue', 'turtle green', 'bright olive', 'dark grey blue', 'greeny brown', 'lemon green', 'light periwinkle', 'seaweed green', 'sunshine yellow', 'ugly purple', 'medium pink', 'puke brown', 'very light pink', 'viridian', 'bile', 'faded yellow', 'very pale green', 'vibrant green', 'bright lime', 'spearmint', 'light aquamarine', 'light sage', 'yellowgreen', 'baby poo', 'dark seafoam', 'deep teal', 'heather', 'rust orange', 'dirty blue', 'fern green', 'bright lilac', 'weird green', 'peacock blue', 'avocado green', 'faded orange', 'grape purple', 'hot green', 'lime yellow', 'mango', 'shamrock', 'bubblegum', 'purplish brown', 'vomit yellow', 'pale cyan', 'key lime', 'tomato red', 'lightgreen', 'merlot', 'night blue', 'purpleish pink', 'apple', 'baby poop green', 'green apple', 'heliotrope', 'yellow/green', 'almost black', 'cool blue', 'leafy green', 'mustard brown', 'dusk', 'dull brown', 'frog green', 'vivid green', 'bright light green', 'fluro green', 'kiwi', 'seaweed', 'navy green', 'ultramarine blue', 'iris', 'pastel orange', 'yellowish orange', 'perrywinkle', 'tealish', 'dark plum', 'pear', 'pinkish orange', 'midnight purple', 'light urple', 'dark mint', 'greenish tan', 'light burgundy', 'turquoise blue', 'ugly pink', 'sandy', 'electric pink', 'muted purple', 'mid green', 'greyish', 'neon yellow', 'banana', 'carnation pink', 'tomato', 'sea', 'muddy brown', 'turquoise green', 'buff', 'fawn', 'muted blue', 'pale rose', 'dark mint green', 'amethyst', 'blue/green', 'chestnut', 'sick green', 'pea', 'rusty orange', 'stone', 'rose red', 'pale aqua', 'deep orange', 'earth', 'mossy green', 'grassy green', 'pale lime green', 'light grey blue', 'pale grey', 'asparagus', 'blueberry', 'purple red', 'pale lime', 'greenish teal', 'caramel', 'deep magenta', 'light peach', 'milk chocolate', 'ocher', 'off green', 'purply pink', 'lightblue', 'dusky blue', 'golden', 'light beige', 'butter yellow', 'dusky purple', 'french blue', 'ugly yellow', 'greeny yellow', 'orangish red', 'shamrock green', 'orangish brown', 'tree green', 'deep violet', 'gunmetal', 'blue/purple', 'cherry', 'sandy brown', 'warm grey', 'dark indigo', 'midnight', 'bluey green', 'grey pink', 'soft purple', 'blood', 'brown red', 'medium grey', 'berry', 'poo', 'purpley pink', 'light salmon', 'snot', 'easter purple', 'light yellow green', 'dark navy blue', 'drab', 'light rose', 'rouge', 'purplish red', 'slime green', 'baby poop', 'irish green', 'pink/purple', 'dark navy', 'greeny blue', 'light plum', 'pinkish grey', 'dirty orange', 'rust red', 'pale lilac', 'orangey red', 'primary blue', 'kermit green', 'brownish purple', 'murky green', 'wheat', 'very dark purple', 'bottle green', 'watermelon', 'deep sky blue', 'fire engine red', 'yellow ochre', 'pumpkin orange', 'pale olive', 'light lilac', 'lightish green', 'carolina blue', 'mulberry', 'shocking pink', 'auburn', 'bright lime green', 'celadon', 'pinkish brown', 'poo brown', 'bright sky blue', 'celery', 'dirt brown', 'strawberry', 'dark lime', 'copper', 'medium brown', 'muted green', \"robin's egg\", 'bright aqua', 'bright lavender', 'ivory', 'very light purple', 'light navy', 'pink red', 'olive brown', 'poop brown', 'mustard green', 'ocean green', 'very dark blue', 'dusty green', 'light navy blue', 'minty green', 'adobe', 'barney', 'jade green', 'bright light blue', 'light lime', 'dark khaki', 'orange yellow', 'ocre', 'maize', 'faded pink', 'british racing green', 'sandstone', 'mud brown', 'light sea green', 'robin egg blue', 'aqua marine', 'dark sea green', 'soft pink', 'orangey brown', 'cherry red', 'burnt yellow', 'brownish grey', 'camel', 'purplish grey', 'marine', 'greyish pink', 'pale turquoise', 'pastel yellow', 'bluey purple', 'canary yellow', 'faded red', 'sepia', 'coffee', 'bright magenta', 'mocha', 'ecru', 'purpleish', 'cranberry', 'darkish green', 'brown orange', 'dusky rose', 'melon', 'sickly green', 'silver', 'purply blue', 'purpleish blue', 'hospital green', 'shit brown', 'mid blue', 'amber', 'easter green', 'soft blue', 'cerulean blue', 'golden brown', 'bright turquoise', 'red pink', 'red purple', 'greyish brown', 'vermillion', 'russet', 'steel grey', 'lighter purple', 'bright violet', 'prussian blue', 'slate green', 'dirty pink', 'dark blue green', 'pine', 'yellowy green', 'dark gold', 'bluish', 'darkish blue', 'dull red', 'pinky red', 'bronze', 'pale teal', 'military green', 'barbie pink', 'bubblegum pink', 'pea soup green', 'dark mustard', 'shit', 'medium purple', 'very dark green', 'dirt', 'dusky pink', 'red violet', 'lemon yellow', 'pistachio', 'dull yellow', 'dark lime green', 'denim blue', 'teal blue', 'lightish blue', 'purpley blue', 'light indigo', 'swamp green', 'brown green', 'dark maroon', 'hot purple', 'dark forest green', 'faded blue', 'drab green', 'light lime green', 'snot green', 'yellowish', 'light blue green', 'bordeaux', 'light mauve', 'ocean', 'marigold', 'muddy green', 'dull orange', 'steel', 'electric purple', 'fluorescent green', 'yellowish brown', 'blush', 'soft green', 'bright orange', 'lemon', 'purple grey', 'acid green', 'pale lavender', 'violet blue', 'light forest green', 'burnt red', 'khaki green', 'cerise', 'faded purple', 'apricot', 'dark olive green', 'grey brown', 'green grey', 'true blue', 'pale violet', 'periwinkle blue', 'light sky blue', 'blurple', 'green brown', 'bluegreen', 'bright teal', 'brownish yellow', 'pea soup', 'forest', 'barney purple', 'ultramarine', 'purplish', 'puke yellow', 'bluish grey', 'dark periwinkle', 'dark lilac', 'reddish', 'light maroon', 'dusty purple', 'terra cotta', 'avocado', 'marine blue', 'teal green', 'slate grey', 'lighter green', 'electric green', 'dusty blue', 'golden yellow', 'bright yellow', 'light lavender', 'umber', 'poop', 'dark peach', 'jungle green', 'eggshell', 'denim', 'yellow brown', 'dull purple', 'chocolate brown', 'wine red', 'neon blue', 'dirty green', 'light tan', 'ice blue', 'cadet blue', 'dark mauve', 'very light blue', 'grey purple', 'pastel pink', 'very light green', 'dark sky blue', 'evergreen', 'dull pink', 'aubergine', 'mahogany', 'reddish orange', 'deep green', 'vomit green', 'purple pink', 'dusty pink', 'faded green', 'camo green', 'pinky purple', 'pink purple', 'brownish red', 'dark rose', 'mud', 'brownish', 'emerald green', 'pale brown', 'dull blue', 'burnt umber', 'medium green', 'clay', 'light aqua', 'light olive green', 'brownish orange', 'dark aqua', 'purplish pink', 'dark salmon', 'greenish grey', 'jade', 'ugly green', 'dark beige', 'emerald', 'pale red', 'light magenta', 'sky', 'light cyan', 'yellow orange', 'reddish purple', 'reddish pink', 'orchid', 'dirty yellow', 'orange red', 'deep red', 'orange brown', 'cobalt blue', 'neon pink', 'rose pink', 'greyish purple', 'raspberry', 'aqua green', 'salmon pink', 'tangerine', 'brownish green', 'red brown', 'greenish brown', 'pumpkin', 'pine green', 'charcoal', 'baby pink', 'cornflower', 'blue violet', 'chocolate', 'greyish green', 'scarlet', 'green yellow', 'dark olive', 'sienna', 'pastel purple', 'terracotta', 'aqua blue', 'sage green', 'blood red', 'deep pink', 'grass', 'moss', 'pastel blue', 'bluish green', 'green blue', 'dark tan', 'greenish blue', 'pale orange', 'vomit', 'forrest green', 'dark lavender', 'dark violet', 'purple blue', 'dark cyan', 'olive drab', 'pinkish', 'cobalt', 'neon purple', 'light turquoise', 'apple green', 'dull green', 'wine', 'powder blue', 'off white', 'electric blue', 'dark turquoise', 'blue purple', 'azure', 'bright red', 'pinkish red', 'cornflower blue', 'light olive', 'grape', 'greyish blue', 'purplish blue', 'yellowish green', 'greenish yellow', 'medium blue', 'dusty rose', 'light violet', 'midnight blue', 'bluish purple', 'red orange', 'dark magenta', 'greenish', 'ocean blue', 'coral', 'cream', 'reddish brown', 'burnt sienna', 'brick', 'sage', 'grey green', 'white', \"robin's egg blue\", 'moss green', 'steel blue', 'eggplant', 'light yellow', 'leaf green', 'light grey', 'puke', 'pinkish purple', 'sea blue', 'pale purple', 'slate blue', 'blue grey', 'hunter green', 'fuchsia', 'crimson', 'pale yellow', 'ochre', 'mustard yellow', 'light red', 'cerulean', 'pale pink', 'deep blue', 'rust', 'light teal', 'slate', 'goldenrod', 'dark yellow', 'dark grey', 'army green', 'grey blue', 'seafoam', 'puce', 'spring green', 'dark orange', 'sand', 'pastel green', 'mint', 'light orange', 'bright pink', 'chartreuse', 'deep purple', 'dark brown', 'taupe', 'pea green', 'puke green', 'kelly green', 'seafoam green', 'blue green', 'khaki', 'burgundy', 'dark teal', 'brick red', 'royal purple', 'plum', 'mint green', 'gold', 'baby blue', 'yellow green', 'bright purple', 'dark red', 'pale blue', 'grass green', 'navy', 'aquamarine', 'burnt orange', 'neon green', 'bright blue', 'rose', 'light pink', 'mustard', 'indigo', 'lime', 'sea green', 'periwinkle', 'dark pink', 'olive green', 'peach', 'pale green', 'light brown', 'hot pink', 'black', 'lilac', 'navy blue', 'royal blue', 'beige', 'salmon', 'olive', 'maroon', 'bright green', 'dark purple', 'mauve', 'forest green', 'aqua', 'cyan', 'tan', 'dark blue', 'lavender', 'turquoise', 'dark green', 'violet', 'light purple', 'lime green', 'grey', 'sky blue', 'yellow', 'magenta', 'light green', 'orange', 'teal', 'light blue', 'red', 'brown', 'pink', 'blue', 'green', 'purple'])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector math\n",
    "\n",
    "Before we keep going, we'll need some functions for performing basic vector arithmetic. These functions will work with vectors in spaces of any number of dimensions.\n",
    "\n",
    "The first function returns the Euclidean distance between two points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:12:21.159984Z",
     "start_time": "2019-12-31T21:12:21.151602Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0990195135927845"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def distance(coord1, coord2):\n",
    "    return np.linalg.norm(np.array(coord1)-np.array(coord2))\n",
    "distance([10, 1], [5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `subtractv` function subtracts one vector from another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:12:21.867874Z",
     "start_time": "2019-12-31T21:12:21.862436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, -1]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subtractv(coord1, coord2):\n",
    "    return list(np.array(coord1) - np.array(coord2))\n",
    "subtractv([10, 1], [5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `addv` vector adds two vectors together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:12:22.488677Z",
     "start_time": "2019-12-31T21:12:22.481474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 3]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def addv(coord1, coord2):\n",
    "    return list(np.array(coord1) + np.array(coord2))\n",
    "addv([10, 1], [5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `meanv` function takes a list of vectors and finds their mean or average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:12:23.283824Z",
     "start_time": "2019-12-31T21:12:23.274436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 2.0]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meanv(coords):\n",
    "    return list(np.mean(coords, axis=0))\n",
    "meanv([[0, 1], [2, 2], [4, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a test, the following cell shows that the distance from \"red\" to \"green\" is greater than the distance from \"red\" to \"pink\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:12:24.156671Z",
     "start_time": "2019-12-31T21:12:24.150587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(colors['red'], colors['green']) > distance(colors['red'], colors['pink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the closest item\n",
    "\n",
    "Just as we wanted to find the animal that most closely matched an arbitrary point in cuteness/size space, we'll want to find the closest color name to an arbitrary point in RGB space. The easiest way to find the closest item to an arbitrary vector is simply to find the distance between the target vector and each item in the space, in turn, then sort the list from closest to farthest. The `closest()` function below does just that. By default, it returns a list of the ten closest items to the given vector.\n",
    "\n",
    "> Note: Calculating \"closest neighbors\" like this is fine for the examples in this notebook, but unmanageably slow for vector spaces of any appreciable size. As your vector space grows, you'll want to move to a faster solution, like SciPy's [kdtree](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.KDTree.html) or [Annoy](https://pypi.python.org/pypi/annoy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:12:25.052509Z",
     "start_time": "2019-12-31T21:12:25.045441Z"
    }
   },
   "outputs": [],
   "source": [
    "def closest(space, coord, n=10):\n",
    "    return sorted(space.keys(), key=lambda x: distance(coord, space[x]))[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it out, we can find the ten colors closest to \"red\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:20:05.727382Z",
     "start_time": "2019-12-31T21:20:05.710934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['white',\n",
       " 'pale grey',\n",
       " 'very light pink',\n",
       " 'off white',\n",
       " 'ice blue',\n",
       " 'very pale blue',\n",
       " 'ice',\n",
       " 'very light blue',\n",
       " 'really light blue',\n",
       " 'eggshell']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, colors['white'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or the ten colors closest to (150, 60, 150):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:20:06.136080Z",
     "start_time": "2019-12-31T21:20:06.115224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barbie pink',\n",
       " 'warm pink',\n",
       " 'medium pink',\n",
       " 'rosy pink',\n",
       " 'darkish pink',\n",
       " 'bubble gum pink',\n",
       " 'strong pink',\n",
       " 'electric pink',\n",
       " 'bubblegum',\n",
       " 'hot pink']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, [255, 60, 150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color magic\n",
    "\n",
    "The magical part of representing words as vectors is that the vector operations we defined earlier appear to operate on language the same way they operate on numbers. For example, if we find the word closest to the vector resulting from subtracting \"red\" from \"purple,\" we get a series of \"blue\" colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:20:06.501668Z",
     "start_time": "2019-12-31T21:20:06.481158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['true green',\n",
       " 'racing green',\n",
       " 'bottle green',\n",
       " 'deep green',\n",
       " 'darkgreen',\n",
       " 'forest',\n",
       " 'emerald green',\n",
       " 'dark green',\n",
       " 'vibrant green',\n",
       " 'british racing green']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, subtractv(colors['green'], colors['purple']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches our intuition about RGB colors, which is that purple is a combination of red and blue. Take away the red, and blue is all you have left.\n",
    "\n",
    "You can do something similar with addition. What's blue plus green?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:20:06.932029Z",
     "start_time": "2019-12-31T21:20:06.914892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bright turquoise',\n",
       " 'bright light blue',\n",
       " 'bright aqua',\n",
       " 'cyan',\n",
       " 'neon blue',\n",
       " 'aqua blue',\n",
       " 'bright cyan',\n",
       " 'bright sky blue',\n",
       " 'aqua',\n",
       " 'bright teal']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, addv(colors['blue'], colors['green']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right, it's something like turquoise or cyan! What if we find the average of black and white? Predictably, we get gray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:20:07.373913Z",
     "start_time": "2019-12-31T21:20:07.354442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medium grey',\n",
       " 'purple grey',\n",
       " 'steel grey',\n",
       " 'battleship grey',\n",
       " 'grey purple',\n",
       " 'purplish grey',\n",
       " 'greyish purple',\n",
       " 'steel',\n",
       " 'warm grey',\n",
       " 'green grey']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the average of black and white: medium grey\n",
    "closest(colors, meanv([colors['black'], colors['white']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the tarantula/hamster example from the previous section, we can use color vectors to reason about relationships between colors. In the cell below, finding the difference between \"pink\" and \"red\" then adding it to \"blue\" seems to give us a list of colors that are to blue what pink is to red (i.e., a slightly lighter, less saturated shade):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:20:07.796339Z",
     "start_time": "2019-12-31T21:20:07.771893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azure',\n",
       " 'turquoise blue',\n",
       " 'bright sky blue',\n",
       " 'aqua blue',\n",
       " 'neon blue',\n",
       " 'water blue',\n",
       " 'topaz',\n",
       " 'cerulean',\n",
       " 'aqua',\n",
       " 'dark sky blue']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an analogy: pink is to red as X is to blue\n",
    "pink_to_red = subtractv(colors['orange'], colors['red'])\n",
    "closest(colors, addv(pink_to_red, colors['blue']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of color analogies: Navy is to blue as true green/dark grass green is to green:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:20:08.254054Z",
     "start_time": "2019-12-31T21:20:08.232477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['true green',\n",
       " 'dark grass green',\n",
       " 'grassy green',\n",
       " 'racing green',\n",
       " 'forest',\n",
       " 'bottle green',\n",
       " 'dark olive green',\n",
       " 'darkgreen',\n",
       " 'forrest green',\n",
       " 'grass green']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another example: \n",
    "navy_to_blue = subtractv(colors['navy'], colors['blue'])\n",
    "closest(colors, addv(navy_to_blue, colors['green']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above are fairly simple from a mathematical perspective but nevertheless *feel* magical: they're demonstrating that it's possible to use math to reason about how people use language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: A Love Poem That Loses Its Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:20:12.083963Z",
     "start_time": "2019-12-31T21:20:11.770351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roses are red, violets are blue\n",
      "Roses are tomato red, violets are electric blue\n",
      "Roses are red orange, violets are azul\n",
      "Roses are orangish red, violets are bright blue\n",
      "Roses are tomato red, violets are deep sky blue\n",
      "Roses are deep orange, violets are bright blue\n",
      "Roses are dark orange, violets are deep sky blue\n",
      "Roses are rust orange, violets are cerulean blue\n",
      "Roses are burnt orange, violets are bright blue\n",
      "Roses are brick orange, violets are electric blue\n",
      "Roses are burnt siena, violets are vibrant blue\n",
      "Roses are orangey brown, violets are bright blue\n",
      "Roses are caramel, violets are electric blue\n",
      "Roses are orange brown, violets are vivid blue\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "red = colors['red']\n",
    "blue = colors['blue']\n",
    "for i in range(14):\n",
    "    rednames = closest(colors, red)\n",
    "    bluenames = closest(colors, blue)\n",
    "    print(f\"Roses are {rednames[0]}, violets are {bluenames[0]}\")\n",
    "    red = colors[random.choice(rednames[1:])]\n",
    "    blue = colors[random.choice(bluenames[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing bad digital humanities with color vectors\n",
    "\n",
    "With the tools above in hand, we can start using our vectorized knowledge of language toward academic ends. In the following example, I'm going to calculate the average color of Bram Stoker's *Dracula*.\n",
    "\n",
    "First, we'll load [spaCy](https://spacy.io/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:12:32.290720Z",
     "start_time": "2019-12-31T21:12:29.605193Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the average color, we'll follow these steps:\n",
    "\n",
    "1. Parse the text into words\n",
    "2. Check every word to see if it names a color in our vector space. If it does, add it to a list of vectors.\n",
    "3. Find the average of that list of vectors.\n",
    "4. Find the color(s) closest to that average vector.\n",
    "\n",
    "The following cell performs steps 1-3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:24:08.012285Z",
     "start_time": "2019-12-31T21:23:20.956889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147.44839067702551, 113.65371809100999, 100.13540510543841]\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get('http://www.gutenberg.org/cache/epub/345/pg345.txt')\n",
    "dracula = nlp(resp.text)\n",
    "# use word.lower_ to normalize case\n",
    "drac_colors = [colors[word.lower_] for word in dracula if word.lower_ in colors]\n",
    "avg_color = meanv(drac_colors)\n",
    "print(avg_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll pass the averaged color vector to the `closest()` function, yielding... well, it's just a brown mush, which is kinda what you'd expect from adding a bunch of colors together willy-nilly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:24:08.031140Z",
     "start_time": "2019-12-31T21:24:08.014447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reddish grey',\n",
       " 'brownish grey',\n",
       " 'brownish',\n",
       " 'brown grey',\n",
       " 'mocha',\n",
       " 'grey brown',\n",
       " 'puce',\n",
       " 'dull brown',\n",
       " 'pinkish brown',\n",
       " 'dark taupe']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, avg_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, here's what we get when we average the colors of Charlotte Perkins Gilman's classic *The Yellow Wallpaper*. The result definitely reflects the content of the story, so maybe we're on to something here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:23:16.251622Z",
     "start_time": "2019-12-31T21:23:13.034901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sickly yellow',\n",
       " 'piss yellow',\n",
       " 'puke yellow',\n",
       " 'vomit yellow',\n",
       " 'dirty yellow',\n",
       " 'mustard yellow',\n",
       " 'dark yellow',\n",
       " 'olive yellow',\n",
       " 'macaroni and cheese',\n",
       " 'pea']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get('http://www.gutenberg.org/cache/epub/1952/pg1952.txt')\n",
    "yellow = nlp(resp.text)\n",
    "wallpaper_colors = [colors[word.lower_] for word in yellow if word.lower_ in colors]\n",
    "avg_color = meanv(wallpaper_colors)\n",
    "closest(colors, avg_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise for the reader: Use the vector arithmetic functions to rewrite a text, making it...\n",
    "\n",
    "* more blue (i.e., add `colors['blue']` to each occurrence of a color word); or\n",
    "* more light (i.e., add `colors['white']` to each occurrence of a color word); or\n",
    "* darker (i.e., attenuate each color. You might need to write a vector multiplication function to do this one right.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional semantics\n",
    "\n",
    "In the previous section, the examples are interesting because of a simple fact: colors that we think of as similar are \"closer\" to each other in RGB vector space. In our color vector space, or in our animal cuteness/size space, you can think of the words identified by vectors close to each other as being *synonyms*, in a sense: they sort of \"mean\" the same thing. They're also, for many purposes, *functionally identical*. Think of this in terms of writing, say, a search engine. If someone searches for \"mauve trousers,\" then it's probably also okay to show them results for, say,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:13:23.088187Z",
     "start_time": "2019-12-31T21:13:23.042156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mauve trousers\n",
      "dusty rose trousers\n",
      "dusky rose trousers\n",
      "brownish pink trousers\n",
      "old pink trousers\n",
      "reddish grey trousers\n",
      "dirty pink trousers\n",
      "old rose trousers\n",
      "light plum trousers\n",
      "ugly pink trousers\n"
     ]
    }
   ],
   "source": [
    "for cname in closest(colors, colors['mauve']):\n",
    "    print(cname, \"trousers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all well and good for color words, which intuitively seem to exist in a multidimensional continuum of perception, and for our animal space, where we've written out the vectors ahead of time. But what about... arbitrary words? Is it possible to create a vector space for all English words that has this same \"closer in space is closer in meaning\" property?\n",
    "\n",
    "To answer that, we have to back up a bit and ask the question: what does *meaning* mean? No one really knows, but one theory popular among computational linguists, computer scientists and other people who make search engines is the [Distributional Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics), which states that:\n",
    "\n",
    "    Linguistic items with similar distributions have similar meanings.\n",
    "    \n",
    "What's meant by \"similar distributions\" is *similar contexts*. Take for example the following sentences:\n",
    "\n",
    "    It was really cold yesterday.\n",
    "    It will be really warm today, though.\n",
    "    It'll be really hot tomorrow!\n",
    "    Will it be really cool Tuesday?\n",
    "    \n",
    "According to the Distributional Hypothesis, the words `cold`, `warm`, `hot` and `cool` must be related in some way (i.e., be close in meaning) because they occur in a similar context, i.e., between the word \"really\" and a word indicating a particular day. (Likewise, the words `yesterday`, `today`, `tomorrow` and `Tuesday` must be related, since they occur in the context of a word indicating a temperature.)\n",
    "\n",
    "In other words, according to the Distributional Hypothesis, a word's meaning is just a big list of all the contexts it occurs in. Two words are closer in meaning if they share contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors by counting contexts\n",
    "\n",
    "So how do we turn this insight from the Distributional Hypothesis into a system for creating general-purpose vectors that capture the meaning of words? Maybe you can see where I'm going with this. What if we made a *really big* spreadsheet that had one column for every context for every word in a given source text. Let's use a small source text to begin with, such as this excerpt from Dickens:\n",
    "\n",
    "    It was the best of times, it was the worst of times.\n",
    "\n",
    "Such a spreadsheet might look something like this:\n",
    "\n",
    "![dickens contexts](http://static.decontextualize.com/snaps/best-of-times.png)\n",
    "\n",
    "The spreadsheet has one column for every possible context, and one row for every word. The values in each cell correspond with how many times the word occurs in the given context. The numbers in the columns constitute that word's vector, i.e., the vector for the word `of` is\n",
    "\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
    "    \n",
    "Because there are ten possible contexts, this is a ten dimensional space! It might be strange to think of it, but you can do vector arithmetic on vectors with ten dimensions just as easily as you can on vectors with two or three dimensions, and you could use the same distance formula that we defined earlier to get useful information about which vectors in this space are similar to each other. In particular, the vectors for `best` and `worst` are actually the same (a distance of zero), since they occur only in the same context (`the ___ of`):\n",
    "\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "Of course, the conventional way of thinking about \"best\" and \"worst\" is that they're *antonyms*, not *synonyms*. But they're also clearly two words of the same kind, with related meanings (through opposition), a fact that is captured by this distributional model.\n",
    "\n",
    "### Contexts and dimensionality\n",
    "\n",
    "Of course, in a corpus of any reasonable size, there will be many thousands if not many millions of possible contexts. It's difficult enough working with a vector space of ten dimensions, let alone a vector space of a million dimensions! It turns out, though, that many of the dimensions end up being superfluous and can either be eliminated or combined with other dimensions without significantly affecting the predictive power of the resulting vectors. The process of getting rid of superfluous dimensions in a vector space is called [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction), and most implementations of count-based word vectors make use of dimensionality reduction so that the resulting vector space has a reasonable number of dimensions (say, 100—300, depending on the corpus and application).\n",
    "\n",
    "The question of how to identify a \"context\" is itself very difficult to answer. In the toy example above, we've said that a \"context\" is just the word that precedes and the word that follows. Depending on your implementation of this procedure, though, you might want a context with a bigger \"window\" (e.g., two words before and after), or a non-contiguous window (skip a word before and after the given word). You might exclude certain \"function\" words like \"the\" and \"of\" when determining a word's context, or you might [lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) the words before you begin your analysis, so two occurrences with different \"forms\" of the same word count as the same context. These are all questions open to research and debate, and different implementations of procedures for creating count-based word vectors make different decisions on this issue.\n",
    "\n",
    "### GloVe vectors\n",
    "\n",
    "But you don't have to create your own word vectors from scratch! Many researchers have made downloadable databases of pre-trained vectors. One such project is Stanford's [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/). These 300-dimensional vectors are included with spaCy, and they're the vectors we'll be using for the rest of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors in spaCy\n",
    "\n",
    "Okay, let's have some fun with real word vectors. We're going to use the GloVe vectors that come with spaCy to creatively analyze and manipulate the text of Bram Stoker's *Dracula*. First, make sure you've got `spacy` imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:25:59.158017Z",
     "start_time": "2019-12-31T21:25:59.152107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hour"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dracula[560]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:25:59.474215Z",
     "start_time": "2019-12-31T21:25:59.462632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.3890932 ,  3.8036323 , -2.6519728 ,  1.6794462 , -0.03245598,\n",
       "        0.34949422,  1.4263201 ,  0.7514826 ,  1.5714422 ,  6.836588  ,\n",
       "       -0.87871516, -2.5962636 ,  4.4980946 , -3.2636712 ,  0.59583217,\n",
       "        1.7193251 , -1.7698543 , -2.6230168 ,  0.32886952, -1.9750781 ,\n",
       "        3.1977305 , -2.3372014 , -2.340062  , -2.59442   , -1.707979  ,\n",
       "        1.4709201 ,  0.46112838, -4.2141914 ,  3.8767452 ,  1.9445885 ,\n",
       "        2.5187936 ,  2.709597  ,  4.9126153 , -2.4080238 ,  1.2024103 ,\n",
       "       -1.7131364 ,  2.7877707 , -4.4838257 , -1.9192042 , -2.094125  ,\n",
       "        3.3191376 ,  2.954278  , -1.6426673 , -2.4524295 , -0.97719395,\n",
       "        1.2865635 , -0.9342208 , -0.07476962, -1.0927347 ,  0.39212835,\n",
       "        2.6349268 , -3.4252431 ,  0.7364484 , -0.04758237, -0.8591391 ,\n",
       "        0.5237043 ,  3.3768516 , -0.6579486 , -0.34316725, -0.7823252 ,\n",
       "        2.7190056 , -2.2406356 , -1.2399366 , -0.10923919,  0.38356325,\n",
       "       -0.07189876,  4.31141   ,  2.5327733 ,  0.69408464, -0.5813782 ,\n",
       "        0.22487086,  1.2595627 , -1.8386738 ,  3.5870411 , -2.5341964 ,\n",
       "       -1.0141946 ,  3.171867  ,  0.07178259,  0.399449  , -0.621662  ,\n",
       "       -0.27902734, -2.2531636 , -4.4507833 ,  1.514179  , -0.8810638 ,\n",
       "        0.2055819 , -2.7417152 ,  1.314493  , -1.0518675 , -1.9557784 ,\n",
       "        1.4149687 ,  1.3410604 , -2.907415  ,  0.14844155,  1.3213062 ,\n",
       "        0.81834567], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:31:37.721649Z",
     "start_time": "2019-12-31T21:27:31.546103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.0/en_core_web_lg-2.2.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.0/en_core_web_lg-2.2.0.tar.gz (827.9MB)\n",
      "Requirement already satisfied: spacy>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from en-core-web-lg==2.2.0) (2.2.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (0.5.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (41.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (1.16.4)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (7.3.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-lg==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-lg==2.2.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-lg==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-lg==2.2.0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-lg==2.2.0) (1.24.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->en-core-web-lg==2.2.0) (1.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.0->en-core-web-lg==2.2.0) (4.32.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->en-core-web-lg==2.2.0) (0.5.1)\n",
      "Building wheels for collected packages: en-core-web-lg\n",
      "  Building wheel for en-core-web-lg (setup.py): started\n",
      "  Building wheel for en-core-web-lg (setup.py): still running...\n",
      "  Building wheel for en-core-web-lg (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\BudBe\\AppData\\Local\\pip\\Cache\\wheels\\9f\\3c\\d6\\3ade7ed8195030f4d7f299cf73d856a84d7b3effd5890133fb\n",
      "Successfully built en-core-web-lg\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-2.2.0\n"
     ]
    }
   ],
   "source": [
    "# previously we've used the _sm model, which doesn't include all vectors.\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.0/en_core_web_lg-2.2.0.tar.gz\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:32:24.216901Z",
     "start_time": "2019-12-31T21:31:37.725549Z"
    }
   },
   "outputs": [],
   "source": [
    "resp = requests.get('http://www.gutenberg.org/cache/epub/345/pg345.txt')\n",
    "dracula = nlp(resp.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cell below creates a list of unique words (or tokens) in the text, as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:32:24.541714Z",
     "start_time": "2019-12-31T21:32:24.221060Z"
    }
   },
   "outputs": [],
   "source": [
    "# all of the words in the text file\n",
    "tokens = list(set([w.text for w in dracula if w.is_alpha]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the vector of any word in spaCy's vocabulary using the `vocab` attribute, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:32:24.556962Z",
     "start_time": "2019-12-31T21:32:24.541714Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.9304e-01,  1.2459e-01,  2.6142e-01,  3.8393e-02,  1.4018e-01,\n",
       "        5.7285e-01, -5.0449e-01,  1.5094e-01,  7.3356e-02,  2.3308e-01,\n",
       "       -1.6048e-01, -5.1184e-01, -1.4028e-01, -3.0110e-01, -4.3446e-01,\n",
       "       -2.8257e-02,  1.9100e-03,  6.5157e-01,  1.4855e-01, -3.7255e-01,\n",
       "       -4.6619e-01, -1.7223e-01, -5.1794e-01, -2.5453e-01, -2.6785e-01,\n",
       "       -6.7776e-02, -3.0085e-01, -3.3212e-01, -1.5862e-01, -1.2336e-01,\n",
       "       -3.8935e-01, -3.5551e-01, -3.6182e-01,  2.3197e-02, -1.7486e-01,\n",
       "        2.2345e-01,  6.7557e-01,  9.6939e-03, -9.5640e-02,  1.2073e-01,\n",
       "       -1.2016e-01, -1.6161e-01,  1.0661e-01, -4.5233e-01,  4.5991e-01,\n",
       "        1.0367e-02, -1.0842e-01, -3.6163e-01,  2.5105e-01,  9.5780e-03,\n",
       "       -6.1363e-01,  2.7494e-01,  2.1783e-01, -1.0663e-01, -9.5146e-03,\n",
       "        2.7570e-01,  2.5888e-02, -5.6749e-02,  6.7667e-03,  5.3735e-01,\n",
       "       -1.2224e-01,  8.8448e-02,  4.6094e-01,  4.6881e-02, -6.6557e-01,\n",
       "       -3.9493e-01,  2.7362e-01,  1.2302e-01,  2.8594e-01, -2.5274e-02,\n",
       "       -8.8458e-02, -6.5280e-01, -1.9723e-01,  2.4883e-01, -7.0509e-01,\n",
       "       -3.9357e-01,  2.1244e-01,  2.8522e-01, -1.3837e-01, -8.0399e-01,\n",
       "        4.0477e-01,  5.5522e-01,  1.3422e-01,  1.7541e-01, -2.4896e-01,\n",
       "       -3.7945e-02,  9.0174e-01,  5.7262e-01,  5.8552e-01, -2.7852e-02,\n",
       "       -3.7543e-01, -2.2329e-01, -1.7649e-01, -2.7149e-01, -1.4442e-01,\n",
       "        1.1173e-01, -7.5823e-01, -3.2636e-02,  3.8259e-01, -4.7711e-01,\n",
       "       -6.4061e-02,  7.7816e-01,  5.2918e-01, -8.8252e-02, -4.8766e-01,\n",
       "       -1.0542e+00, -1.7567e-01,  5.0561e-01, -3.2961e-01,  6.4652e-02,\n",
       "        1.6991e-01,  6.4564e-01, -1.1113e-01, -2.7329e-01,  4.5151e-01,\n",
       "        2.0905e-01,  5.5951e-01, -5.6235e-01,  6.4065e-01,  2.6138e-01,\n",
       "       -7.6238e-02, -5.7198e-01,  5.7286e-01, -6.4884e-01, -3.5580e-02,\n",
       "       -2.9065e-01, -2.3659e-01, -1.5837e-01, -3.8830e-01,  3.5099e-01,\n",
       "        1.2338e-01, -7.7062e-01, -3.2580e-01,  5.4685e-01,  9.9670e-02,\n",
       "       -1.7511e-01,  5.9578e-01,  5.7704e-02,  3.6616e-01,  7.9128e-02,\n",
       "       -2.1015e+00,  2.9241e-01,  1.6590e-01, -1.1812e-01,  3.6034e-01,\n",
       "        2.1378e-01, -3.8785e-02, -1.6689e-01,  2.5772e-01,  1.4423e-01,\n",
       "       -3.2560e-01, -4.9781e-01, -2.4736e-01, -4.5972e-01,  5.3241e-03,\n",
       "       -3.8934e-01, -5.6316e-01, -2.1875e-02,  4.6312e-01,  2.7124e-01,\n",
       "        8.2626e-02,  1.2798e-01, -1.7966e-01, -4.0142e-01,  3.9830e-01,\n",
       "        4.7246e-01, -1.7184e-02,  3.0340e-01,  7.2356e-01,  6.2398e-03,\n",
       "        1.1034e-01,  4.3798e-01, -1.8209e-01,  7.9000e-01, -8.2937e-01,\n",
       "       -2.5152e-01, -3.5818e-01,  1.9120e-01,  2.6807e-01, -4.8448e-02,\n",
       "        4.4986e-01, -1.9426e-01, -4.1831e-01, -4.7271e-01,  1.8365e-02,\n",
       "       -5.3575e-02,  2.0164e-01,  1.4613e-01, -5.2150e-01, -3.5136e-01,\n",
       "       -5.6628e-01,  5.6513e-01,  1.5970e-01,  4.9475e-01,  7.8591e-02,\n",
       "        5.8238e-01,  5.5110e-01,  4.7882e-01,  5.1979e-02, -5.8825e-01,\n",
       "       -3.1104e-01,  1.9566e-01, -3.4389e-01,  5.2565e-01,  2.7204e-01,\n",
       "       -1.9269e-01,  4.0240e-01,  3.5336e-01,  1.1571e-01, -1.7639e-01,\n",
       "        3.6528e-01,  4.7704e-01, -7.1949e-02, -2.3349e-01, -3.6303e-01,\n",
       "        1.8091e-01, -6.1703e-01,  1.0734e-01,  2.0010e-01,  4.5323e-02,\n",
       "       -1.4448e-01,  2.3747e-01, -1.0277e-01, -5.3455e-01,  1.3774e-01,\n",
       "       -2.1872e-01,  2.9646e-01,  8.5221e-01,  1.0667e-01,  8.3190e-02,\n",
       "        2.1841e-01, -2.0192e-01, -9.4416e-02,  1.7515e-01,  4.4191e-01,\n",
       "        5.8381e-01, -3.2412e-01, -2.3751e-01,  6.5186e-01,  3.9159e-01,\n",
       "        2.4443e-01, -6.5128e-01, -3.2916e-01, -5.9612e-01, -2.8962e-01,\n",
       "        1.2222e-01, -2.6307e-02,  1.1961e-01, -2.7490e-01,  2.4727e-01,\n",
       "       -1.0389e-01,  3.8307e-01, -5.0076e-01,  4.6286e-01,  4.6927e-01,\n",
       "       -3.5431e-01,  5.9146e-01,  2.0201e-01,  8.0544e-01,  2.7008e-01,\n",
       "       -5.9786e-02, -2.7738e-01, -2.3822e-01, -5.1769e-02,  2.1425e-01,\n",
       "        2.5170e-01,  1.8265e-01, -2.4323e-01, -3.2770e-01,  1.5040e-02,\n",
       "       -3.8119e-01, -5.2011e-02,  4.2345e-01,  6.3603e-01,  5.0159e-01,\n",
       "       -1.6942e-01, -2.3502e-01, -2.0667e-01, -9.0480e-02, -9.7789e-01,\n",
       "       -2.8939e-01,  2.2706e-01, -1.2613e-01,  4.4867e-02,  3.8184e-01,\n",
       "       -3.1982e-01, -1.4018e-01, -1.3953e-02, -4.4110e-01, -4.3323e-01,\n",
       "        4.6663e-01,  2.8850e-01, -8.1727e-01, -4.4984e-01,  8.9386e-02,\n",
       "        9.0084e-01, -5.8482e-02,  5.1920e-02,  1.5626e-02, -7.1096e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['alligator'].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, the following function gets the vector of a given string from spaCy's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T21:32:24.570041Z",
     "start_time": "2019-12-31T21:32:24.558956Z"
    }
   },
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp.vocab[s].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity and finding closest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows that the cosine similarity between `dog` and `puppy` is larger than the similarity between `trousers` and `octopus`, thereby demonstrating that the vectors are working how we expect them to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cosine_similarity(vec('dog').reshape(1, -1), vec('puppy').reshape(1, -1))[0][0] > \n",
    "    cosine_similarity(vec('trousers').reshape(1, -1), vec('octopus').reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines a function that iterates through a list of tokens and returns the token whose vector is most similar to a given vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list,\n",
    "                  key=lambda x: cosine_similarity(vec_to_check.reshape(1, -1), \n",
    "                                                  vec(x).reshape(1, -1))[0][0],\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can get a list of synonyms, or words closest in meaning (or distribution, depending on how you look at it), to any arbitrary word in spaCy's vocabulary. In the following example, we're finding the words in *Dracula* closest to \"basketball\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what's the closest equivalent of basketball?\n",
    "spacy_closest(tokens, vec(\"basketball\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun with spaCy, Dracula, and vector arithmetic\n",
    "\n",
    "Now we can start doing vector arithmetic and finding the closest words to the resulting vectors. For example, what word is closest to the halfway point between day and night?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# halfway between day and night\n",
    "spacy_closest(tokens, np.array(meanv([vec(\"day\"), vec(\"night\")])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variations of `night` and `day` are still closest, but after that we get words like `evening` and `morning`, which are indeed halfway between day and night!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the closest words in _Dracula_ to \"wine\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens, vec(\"wine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you subtract \"alcohol\" from \"wine\" and find the closest words to the resulting vector, you're left with simply a lovely dinner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens, np.array(subtractv(vec(\"wine\"), vec(\"alcohol\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closest words to \"water\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens, vec(\"water\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you add \"frozen\" to \"water,\" you get \"ice\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens, np.array(addv(vec(\"water\"), vec(\"frozen\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even do analogies! For example, the words most similar to \"grass\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spacy_closest(tokens, vec(\"grass\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take the difference of \"blue\" and \"sky\" and add it to grass, you get the analogous word (\"green\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# analogy: blue is to sky as X is to grass\n",
    "blue_to_sky = subtractv(vec(\"blue\"), vec(\"sky\"))\n",
    "spacy_closest(tokens, np.array(addv(blue_to_sky, vec(\"grass\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further resources\n",
    "\n",
    "* [Word2vec](https://en.wikipedia.org/wiki/Word2vec) is another procedure for producing word vectors which uses a predictive approach rather than a context-counting approach. [This paper](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf) compares and contrasts the two approaches. (Spoiler: it's kind of a wash.)\n",
    "* If you want to train your own word vectors on a particular corpus, the popular Python library [gensim](https://radimrehurek.com/gensim/) has an implementation of Word2Vec that is relatively easy to use. [There's a good tutorial here.](https://rare-technologies.com/word2vec-tutorial/)\n",
    "* When you're working with vector spaces with high dimensionality and millions of vectors, iterating through your entire space calculating cosine similarities can be a drag. I use [Annoy](https://pypi.python.org/pypi/annoy) to make these calculations faster, and you should consider using it too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
